{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Resume Search MCP API - Comprehensive Testing Suite\n",
    "\n",
    "This notebook provides comprehensive testing and demonstration of the HR Resume Search MCP API.\n",
    "\n",
    "## Features Tested\n",
    "- ‚úÖ **Authentication Flow**: JWT login, token management, refresh\n",
    "- ‚úÖ **Resume Upload**: PDF/DOC/DOCX file processing with Claude AI\n",
    "- ‚úÖ **Search Functionality**: Smart candidate matching and filtering\n",
    "- ‚úÖ **Performance Testing**: Concurrent requests and load testing\n",
    "- ‚úÖ **Data Visualization**: Search results and performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "1. API server running at `http://localhost:8000`\n",
    "2. Database and Redis services available\n",
    "3. Claude API key configured\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import time\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# IPython display\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed, IntSlider\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configure seaborn\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_HOST = os.getenv('API_HOST', 'localhost')\n",
    "API_PORT = os.getenv('API_PORT', '8000')\n",
    "API_PREFIX = os.getenv('API_PREFIX', '/api/v1')\n",
    "API_BASE_URL = f\"http://{API_HOST}:{API_PORT}\"\n",
    "API_URL = f\"{API_BASE_URL}{API_PREFIX}\"\n",
    "\n",
    "# Test configuration\n",
    "TEST_TIMEOUT = 30\n",
    "CONCURRENT_REQUESTS = 10\n",
    "LOAD_TEST_DURATION = 60  # seconds\n",
    "\n",
    "# Display configuration\n",
    "config_html = f\"\"\"\n",
    "<div style=\"background-color: #f0f8ff; padding: 15px; border-radius: 10px; border-left: 5px solid #007acc;\">\n",
    "    <h3 style=\"color: #007acc; margin-top: 0;\">üîß API Configuration</h3>\n",
    "    <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr><td><strong>Base URL:</strong></td><td>{API_BASE_URL}</td></tr>\n",
    "        <tr><td><strong>API URL:</strong></td><td>{API_URL}</td></tr>\n",
    "        <tr><td><strong>Timeout:</strong></td><td>{TEST_TIMEOUT}s</td></tr>\n",
    "        <tr><td><strong>Environment:</strong></td><td>{os.getenv('ENVIRONMENT', 'development')}</td></tr>\n",
    "    </table>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(config_html))\n",
    "print(f\"API Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Helper Functions and Test Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APITestClient:\n",
    "    \"\"\"Enhanced API testing client with authentication and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = API_URL):\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.Client(timeout=TEST_TIMEOUT)\n",
    "        self.async_client = None\n",
    "        self.auth_token = None\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        self.metrics = {\n",
    "            \"requests_made\": 0,\n",
    "            \"requests_successful\": 0,\n",
    "            \"requests_failed\": 0,\n",
    "            \"total_response_time\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def set_auth_token(self, token: str):\n",
    "        \"\"\"Set authentication token\"\"\"\n",
    "        self.auth_token = token\n",
    "        self.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    \n",
    "    def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Make HTTP request with metrics tracking\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\" if not endpoint.startswith('http') else endpoint\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = getattr(self.client, method.lower())(\n",
    "                url, headers=self.headers, **kwargs\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics[\"requests_made\"] += 1\n",
    "            self.metrics[\"total_response_time\"] += response_time\n",
    "            \n",
    "            if response.status_code < 400:\n",
    "                self.metrics[\"requests_successful\"] += 1\n",
    "            else:\n",
    "                self.metrics[\"requests_failed\"] += 1\n",
    "                self.metrics[\"errors\"].append({\n",
    "                    \"endpoint\": endpoint,\n",
    "                    \"status_code\": response.status_code,\n",
    "                    \"error\": response.text\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code < 400,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": response.json() if response.content else None,\n",
    "                \"headers\": dict(response.headers),\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            self.metrics[\"requests_made\"] += 1\n",
    "            self.metrics[\"requests_failed\"] += 1\n",
    "            self.metrics[\"total_response_time\"] += response_time\n",
    "            self.metrics[\"errors\"].append({\n",
    "                \"endpoint\": endpoint,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "    \n",
    "    def get(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"GET\", endpoint, **kwargs)\n",
    "    \n",
    "    def post(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"POST\", endpoint, **kwargs)\n",
    "    \n",
    "    def put(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"PUT\", endpoint, **kwargs)\n",
    "    \n",
    "    def delete(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"DELETE\", endpoint, **kwargs)\n",
    "    \n",
    "    async def async_get(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Async GET request\"\"\"\n",
    "        if not self.async_client:\n",
    "            self.async_client = httpx.AsyncClient(timeout=TEST_TIMEOUT)\n",
    "        \n",
    "        url = f\"{self.base_url}{endpoint}\" if not endpoint.startswith('http') else endpoint\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = await self.async_client.get(\n",
    "                url, headers=self.headers, **kwargs\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code < 400,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": response.json() if response.content else None,\n",
    "                \"url\": url\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance metrics\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        if metrics[\"requests_made\"] > 0:\n",
    "            metrics[\"average_response_time\"] = metrics[\"total_response_time\"] / metrics[\"requests_made\"]\n",
    "            metrics[\"success_rate\"] = (metrics[\"requests_successful\"] / metrics[\"requests_made\"]) * 100\n",
    "        else:\n",
    "            metrics[\"average_response_time\"] = 0\n",
    "            metrics[\"success_rate\"] = 0\n",
    "        return metrics\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset performance metrics\"\"\"\n",
    "        self.metrics = {\n",
    "            \"requests_made\": 0,\n",
    "            \"requests_successful\": 0,\n",
    "            \"requests_failed\": 0,\n",
    "            \"total_response_time\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close client connections\"\"\"\n",
    "        self.client.close()\n",
    "        if self.async_client:\n",
    "            asyncio.run(self.async_client.aclose())\n",
    "\n",
    "# Initialize test client\n",
    "api_client = APITestClient()\n",
    "\n",
    "print(\"‚úÖ APITestClient initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_resume_files():\n",
    "    \"\"\"Create sample resume files for testing\"\"\"\n",
    "    sample_resumes = {\n",
    "        \"john_doe_resume.json\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"email\": \"john.doe@example.com\",\n",
    "            \"phone\": \"+1-555-0123\",\n",
    "            \"location\": \"New York, NY\",\n",
    "            \"summary\": \"Experienced software engineer with 5+ years in full-stack development\",\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"company\": \"Tech Corp\",\n",
    "                    \"position\": \"Senior Software Engineer\",\n",
    "                    \"department\": \"Engineering\",\n",
    "                    \"desk\": \"Platform Team\",\n",
    "                    \"start_date\": \"2020-01-01\",\n",
    "                    \"end_date\": None,\n",
    "                    \"description\": \"Leading platform development initiatives\"\n",
    "                },\n",
    "                {\n",
    "                    \"company\": \"StartupXYZ\",\n",
    "                    \"position\": \"Software Engineer\",\n",
    "                    \"department\": \"Product\",\n",
    "                    \"desk\": \"Backend Team\",\n",
    "                    \"start_date\": \"2018-06-01\",\n",
    "                    \"end_date\": \"2019-12-31\",\n",
    "                    \"description\": \"Developed scalable backend services\"\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"Docker\", \"AWS\", \"PostgreSQL\"],\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"institution\": \"University of Technology\",\n",
    "                    \"degree\": \"Bachelor of Science\",\n",
    "                    \"field_of_study\": \"Computer Science\",\n",
    "                    \"graduation_date\": \"2018-05-01\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"jane_smith_resume.json\": {\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"email\": \"jane.smith@example.com\",\n",
    "            \"phone\": \"+1-555-0456\",\n",
    "            \"location\": \"San Francisco, CA\",\n",
    "            \"summary\": \"Product manager with expertise in data analytics and machine learning\",\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"company\": \"Tech Corp\",\n",
    "                    \"position\": \"Senior Product Manager\",\n",
    "                    \"department\": \"Product\",\n",
    "                    \"desk\": \"AI Team\",\n",
    "                    \"start_date\": \"2019-03-01\",\n",
    "                    \"end_date\": None,\n",
    "                    \"description\": \"Leading AI product initiatives\"\n",
    "                },\n",
    "                {\n",
    "                    \"company\": \"DataCorp\",\n",
    "                    \"position\": \"Data Analyst\",\n",
    "                    \"department\": \"Analytics\",\n",
    "                    \"desk\": \"Business Intelligence\",\n",
    "                    \"start_date\": \"2017-01-01\",\n",
    "                    \"end_date\": \"2019-02-28\",\n",
    "                    \"description\": \"Analyzed business metrics and created dashboards\"\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\"Python\", \"SQL\", \"Tableau\", \"Machine Learning\", \"Product Strategy\"],\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"institution\": \"Stanford University\",\n",
    "                    \"degree\": \"Master of Science\",\n",
    "                    \"field_of_study\": \"Data Science\",\n",
    "                    \"graduation_date\": \"2016-06-01\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create sample files directory\n",
    "    sample_dir = Path(\"sample_resumes\")\n",
    "    sample_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save sample files\n",
    "    for filename, data in sample_resumes.items():\n",
    "        with open(sample_dir / filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    return sample_dir, list(sample_resumes.keys())\n",
    "\n",
    "def visualize_test_results(results: List[Dict], title: str = \"Test Results\"):\n",
    "    \"\"\"Visualize test results with multiple charts\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Response Times', 'Success Rate', 'Status Codes', 'Timeline'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Response times histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df['response_time'], name=\"Response Time\", nbinsx=20),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Success rate pie chart\n",
    "    success_counts = df['success'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=['Success', 'Failed'], values=[success_counts.get(True, 0), success_counts.get(False, 0)]),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Status codes bar chart\n",
    "    status_counts = df['status_code'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=status_counts.index.astype(str), y=status_counts.values, name=\"Status Codes\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Timeline scatter plot\n",
    "    if 'timestamp' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df.index, y=df['response_time'], mode='lines+markers', name=\"Response Time Timeline\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=title, showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    stats_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "        <h4>üìä Test Summary Statistics</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px;\">\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #007acc;\">{len(df)}</div>\n",
    "                <div>Total Requests</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #28a745;\">{df['success'].sum()}</div>\n",
    "                <div>Successful</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #dc3545;\">{(~df['success']).sum()}</div>\n",
    "                <div>Failed</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #6f42c1;\">{df['response_time'].mean():.3f}s</div>\n",
    "                <div>Avg Response Time</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(stats_html))\n",
    "\n",
    "# Create sample data\n",
    "sample_dir, sample_files = create_sample_resume_files()\n",
    "print(f\"‚úÖ Created sample resume files: {sample_files}\")\n",
    "print(f\"‚úÖ Helper functions initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç API Health Check\n",
    "\n",
    "First, let's verify that the API is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic API connectivity\n",
    "print(\"üîç Testing API connectivity...\")\n",
    "\n",
    "health_endpoints = [\n",
    "    (\"/\", \"Root endpoint\"),\n",
    "    (\"/health\", \"Health check\"),\n",
    "    (\"/readiness\", \"Readiness check\"),\n",
    "    (\"/docs\", \"API documentation\"),\n",
    "    (\"/openapi.json\", \"OpenAPI schema\")\n",
    "]\n",
    "\n",
    "health_results = []\n",
    "\n",
    "for endpoint, description in health_endpoints:\n",
    "    # Use base URL for root-level endpoints\n",
    "    test_url = f\"{API_BASE_URL}{endpoint}\" if endpoint in [\"/\", \"/health\", \"/readiness\", \"/docs\", \"/openapi.json\"] else endpoint\n",
    "    result = api_client.get(test_url)\n",
    "    \n",
    "    health_results.append({\n",
    "        \"endpoint\": endpoint,\n",
    "        \"description\": description,\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"success\": result[\"success\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} {endpoint} ({description}): {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "# Visualize health check results\n",
    "visualize_test_results(health_results, \"API Health Check Results\")\n",
    "\n",
    "# Check if API is ready\n",
    "api_ready = all(result[\"success\"] for result in health_results if result[\"endpoint\"] in [\"/\", \"/health\"])\n",
    "if api_ready:\n",
    "    print(\"\\nüéâ API is ready for testing!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è API may not be fully ready. Some tests may fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîê Authentication Flow Testing\n",
    "\n",
    "Testing JWT authentication, login, token refresh, and logout flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication test data\n",
    "test_users = {\n",
    "    \"admin\": {\n",
    "        \"email\": \"admin@example.com\",\n",
    "        \"password\": \"admin123\",\n",
    "        \"role\": \"admin\"\n",
    "    },\n",
    "    \"hr_manager\": {\n",
    "        \"email\": \"hr@example.com\",\n",
    "        \"password\": \"hr123\",\n",
    "        \"role\": \"hr_manager\"\n",
    "    },\n",
    "    \"recruiter\": {\n",
    "        \"email\": \"recruiter@example.com\",\n",
    "        \"password\": \"recruiter123\",\n",
    "        \"role\": \"recruiter\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîê Testing Authentication Flow...\")\n",
    "auth_results = []\n",
    "\n",
    "# Test user registration (if endpoint exists)\n",
    "print(\"\\n1. Testing User Registration\")\n",
    "for username, user_data in test_users.items():\n",
    "    register_data = {\n",
    "        \"email\": user_data[\"email\"],\n",
    "        \"password\": user_data[\"password\"],\n",
    "        \"role\": user_data[\"role\"]\n",
    "    }\n",
    "    \n",
    "    result = api_client.post(\"/auth/register\", json=register_data)\n",
    "    auth_results.append({\n",
    "        \"test\": f\"Register {username}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] or result[\"status_code\"] == 409 else \"‚ùå\"  # 409 = user exists\n",
    "    print(f\"{status_icon} Register {username}: {result['status_code']}\")\n",
    "\n",
    "# Test login\n",
    "print(\"\\n2. Testing User Login\")\n",
    "successful_logins = {}\n",
    "\n",
    "for username, user_data in test_users.items():\n",
    "    login_data = {\n",
    "        \"email\": user_data[\"email\"],\n",
    "        \"password\": user_data[\"password\"]\n",
    "    }\n",
    "    \n",
    "    result = api_client.post(\"/auth/login\", json=login_data)\n",
    "    auth_results.append({\n",
    "        \"test\": f\"Login {username}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        token = result[\"data\"].get(\"access_token\")\n",
    "        if token:\n",
    "            successful_logins[username] = {\n",
    "                \"token\": token,\n",
    "                \"user_data\": result[\"data\"]\n",
    "            }\n",
    "            print(f\"‚úÖ Login {username}: Success - Token obtained\")\n",
    "        else:\n",
    "            print(f\"‚ùå Login {username}: No token in response\")\n",
    "    else:\n",
    "        print(f\"‚ùå Login {username}: {result.get('status_code', 'Unknown error')}\")\n",
    "\n",
    "# Test authenticated endpoints\n",
    "print(\"\\n3. Testing Authenticated Endpoints\")\n",
    "if successful_logins:\n",
    "    # Use first successful login for testing\n",
    "    test_user = list(successful_logins.keys())[0]\n",
    "    test_token = successful_logins[test_user][\"token\"]\n",
    "    \n",
    "    # Set token for API client\n",
    "    api_client.set_auth_token(test_token)\n",
    "    \n",
    "    # Test protected endpoints\n",
    "    protected_endpoints = [\n",
    "        \"/auth/me\",\n",
    "        \"/resumes\",\n",
    "        \"/search\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in protected_endpoints:\n",
    "        result = api_client.get(endpoint)\n",
    "        auth_results.append({\n",
    "            \"test\": f\"Protected {endpoint}\",\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} {endpoint}: {result['status_code']}\")\n",
    "    \n",
    "    print(f\"\\nüîë Using token for {test_user}: {test_token[:20]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No successful logins - cannot test protected endpoints\")\n",
    "\n",
    "# Display authentication test results\n",
    "print(\"\\nüìä Authentication Test Summary:\")\n",
    "visualize_test_results(auth_results, \"Authentication Flow Test Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Resume Upload Testing\n",
    "\n",
    "Testing file upload functionality with various formats and Claude AI parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÑ Testing Resume Upload Functionality...\")\n",
    "upload_results = []\n",
    "\n",
    "# Create test files in different formats\n",
    "def create_test_resume_content():\n",
    "    return \"\"\"\n",
    "JOHN DOE\n",
    "Software Engineer\n",
    "Email: john.doe@example.com\n",
    "Phone: +1-555-0123\n",
    "Location: New York, NY\n",
    "\n",
    "SUMMARY\n",
    "Experienced software engineer with 5+ years in full-stack development.\n",
    "Expertise in Python, JavaScript, and cloud technologies.\n",
    "\n",
    "EXPERIENCE\n",
    "Senior Software Engineer | Tech Corp | 2020-Present\n",
    "- Lead platform development initiatives\n",
    "- Manage team of 5 developers\n",
    "- Implemented microservices architecture\n",
    "\n",
    "Software Engineer | StartupXYZ | 2018-2019\n",
    "- Developed scalable backend services\n",
    "- Built REST APIs and databases\n",
    "- Worked with Docker and Kubernetes\n",
    "\n",
    "SKILLS\n",
    "Programming: Python, JavaScript, Go, SQL\n",
    "Technologies: Docker, Kubernetes, AWS, PostgreSQL\n",
    "Frameworks: FastAPI, React, Node.js\n",
    "\n",
    "EDUCATION\n",
    "Bachelor of Science in Computer Science\n",
    "University of Technology | 2018\n",
    "\"\"\"\n",
    "\n",
    "# Create test files\n",
    "test_files_dir = Path(\"test_uploads\")\n",
    "test_files_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create text file (simulating converted PDF content)\n",
    "test_content = create_test_resume_content()\n",
    "test_files = []\n",
    "\n",
    "# Text file\n",
    "text_file = test_files_dir / \"john_doe_resume.txt\"\n",
    "with open(text_file, 'w') as f:\n",
    "    f.write(test_content)\n",
    "test_files.append((\"john_doe_resume.txt\", \"text/plain\"))\n",
    "\n",
    "# JSON file (structured resume)\n",
    "json_file = test_files_dir / \"jane_smith_resume.json\"\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Jane Smith\",\n",
    "        \"email\": \"jane.smith@example.com\",\n",
    "        \"position\": \"Product Manager\",\n",
    "        \"experience\": [\n",
    "            {\n",
    "                \"company\": \"Tech Corp\",\n",
    "                \"role\": \"Senior Product Manager\",\n",
    "                \"department\": \"Product\",\n",
    "                \"years\": 3\n",
    "            }\n",
    "        ],\n",
    "        \"skills\": [\"Product Strategy\", \"Data Analysis\", \"Python\", \"SQL\"]\n",
    "    }, f, indent=2)\n",
    "test_files.append((\"jane_smith_resume.json\", \"application/json\"))\n",
    "\n",
    "print(f\"Created test files: {[f[0] for f in test_files]}\")\n",
    "\n",
    "# Test file uploads\n",
    "print(\"\\n1. Testing File Upload Endpoints\")\n",
    "uploaded_resumes = []\n",
    "\n",
    "for filename, content_type in test_files:\n",
    "    file_path = test_files_dir / filename\n",
    "    \n",
    "    # Test upload\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            files = {'file': (filename, f, content_type)}\n",
    "            # Note: files parameter bypasses the json content-type header\n",
    "            result = api_client.client.post(\n",
    "                f\"{api_client.base_url}/resumes/upload\",\n",
    "                files=files,\n",
    "                headers={\"Authorization\": api_client.headers.get(\"Authorization\", \"\")}\n",
    "            )\n",
    "            \n",
    "            upload_result = {\n",
    "                \"success\": result.status_code < 400,\n",
    "                \"status_code\": result.status_code,\n",
    "                \"response_time\": 0,  # Will be updated by client metrics\n",
    "                \"data\": result.json() if result.content else None,\n",
    "                \"filename\": filename\n",
    "            }\n",
    "            \n",
    "            upload_results.append({\n",
    "                \"test\": f\"Upload {filename}\",\n",
    "                \"success\": upload_result[\"success\"],\n",
    "                \"status_code\": upload_result[\"status_code\"],\n",
    "                \"response_time\": upload_result[\"response_time\"],\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"file_type\": content_type\n",
    "            })\n",
    "            \n",
    "            if upload_result[\"success\"] and upload_result[\"data\"]:\n",
    "                resume_id = upload_result[\"data\"].get(\"id\")\n",
    "                if resume_id:\n",
    "                    uploaded_resumes.append({\n",
    "                        \"id\": resume_id,\n",
    "                        \"filename\": filename,\n",
    "                        \"data\": upload_result[\"data\"]\n",
    "                    })\n",
    "                    print(f\"‚úÖ Uploaded {filename}: ID {resume_id}\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ Uploaded {filename}: No ID returned\")\n",
    "            else:\n",
    "                print(f\"‚ùå Upload {filename}: {upload_result['status_code']}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload {filename}: Error - {str(e)}\")\n",
    "        upload_results.append({\n",
    "            \"test\": f\"Upload {filename}\",\n",
    "            \"success\": False,\n",
    "            \"status_code\": None,\n",
    "            \"response_time\": 0,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Test resume parsing status\n",
    "print(\"\\n2. Testing Resume Parsing Status\")\n",
    "for resume in uploaded_resumes:\n",
    "    result = api_client.get(f\"/resumes/{resume['id']}\")\n",
    "    upload_results.append({\n",
    "        \"test\": f\"Get resume {resume['id']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        parse_status = result[\"data\"].get(\"parse_status\", \"unknown\")\n",
    "        print(f\"‚úÖ Resume {resume['id']}: Parse status = {parse_status}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Resume {resume['id']}: Could not get status\")\n",
    "\n",
    "# Test Claude AI parsing (if available)\n",
    "print(\"\\n3. Testing Claude AI Parsing\")\n",
    "for resume in uploaded_resumes:\n",
    "    result = api_client.get(f\"/resumes/{resume['id']}/parse\")\n",
    "    upload_results.append({\n",
    "        \"test\": f\"Parse resume {resume['id']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} Parse resume {resume['id']}: {result['status_code']}\")\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        parsed_data = result[\"data\"]\n",
    "        print(f\"   Parsed fields: {list(parsed_data.keys())}\")\n",
    "\n",
    "# Display upload test results\n",
    "print(\"\\nüìä Resume Upload Test Summary:\")\n",
    "visualize_test_results(upload_results, \"Resume Upload Test Results\")\n",
    "\n",
    "# Store uploaded resume IDs for search testing\n",
    "resume_ids = [r[\"id\"] for r in uploaded_resumes]\n",
    "print(f\"\\nüìù Uploaded resume IDs for search testing: {resume_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Search Functionality Testing\n",
    "\n",
    "Testing various search capabilities including smart matching and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Testing Search Functionality...\")\n",
    "search_results = []\n",
    "search_data = []\n",
    "\n",
    "# Define test search queries\n",
    "search_queries = [\n",
    "    {\n",
    "        \"name\": \"Basic keyword search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"q\": \"software engineer\"},\n",
    "        \"description\": \"Search for software engineers\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Skills-based search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"skills\": \"Python,JavaScript\"},\n",
    "        \"description\": \"Search by specific skills\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Department search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"department\": \"Engineering\"},\n",
    "        \"description\": \"Search by department\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Experience range\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"min_experience\": 3, \"max_experience\": 8},\n",
    "        \"description\": \"Search by experience range\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Location search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"location\": \"New York\"},\n",
    "        \"description\": \"Search by location\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Advanced search\",\n",
    "        \"endpoint\": \"/search/advanced\",\n",
    "        \"method\": \"POST\",\n",
    "        \"data\": {\n",
    "            \"query\": \"senior engineer\",\n",
    "            \"filters\": {\n",
    "                \"skills\": [\"Python\", \"AWS\"],\n",
    "                \"experience_years\": {\"min\": 3, \"max\": 10},\n",
    "                \"departments\": [\"Engineering\", \"Product\"]\n",
    "            },\n",
    "            \"sort_by\": \"relevance\",\n",
    "            \"limit\": 20\n",
    "        },\n",
    "        \"description\": \"Advanced search with multiple filters\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test similar candidates search (if we have uploaded resumes)\n",
    "if resume_ids:\n",
    "    for resume_id in resume_ids[:2]:  # Test first 2 resumes\n",
    "        search_queries.append({\n",
    "            \"name\": f\"Similar candidates to {resume_id}\",\n",
    "            \"endpoint\": f\"/search/similar/{resume_id}\",\n",
    "            \"params\": {},\n",
    "            \"description\": f\"Find candidates similar to resume {resume_id}\"\n",
    "        })\n",
    "\n",
    "# Execute search tests\n",
    "print(\"\\n1. Testing Search Endpoints\")\n",
    "for query in search_queries:\n",
    "    try:\n",
    "        if query.get(\"method\") == \"POST\":\n",
    "            result = api_client.post(query[\"endpoint\"], json=query.get(\"data\", {}))\n",
    "        else:\n",
    "            result = api_client.get(query[\"endpoint\"], params=query.get(\"params\", {}))\n",
    "        \n",
    "        search_results.append({\n",
    "            \"test\": query[\"name\"],\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"endpoint\": query[\"endpoint\"]\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} {query['name']}: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "        \n",
    "        # Store search data for visualization\n",
    "        if result[\"success\"] and result[\"data\"]:\n",
    "            results_data = result[\"data\"].get(\"results\", [])\n",
    "            total_count = result[\"data\"].get(\"total\", 0)\n",
    "            \n",
    "            search_data.append({\n",
    "                \"query_name\": query[\"name\"],\n",
    "                \"total_results\": total_count,\n",
    "                \"returned_results\": len(results_data),\n",
    "                \"response_time\": result[\"response_time\"],\n",
    "                \"results\": results_data[:5]  # Store first 5 results for analysis\n",
    "            })\n",
    "            \n",
    "            print(f\"   Found {total_count} total results, returned {len(results_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {query['name']}: Error - {str(e)}\")\n",
    "        search_results.append({\n",
    "            \"test\": query[\"name\"],\n",
    "            \"success\": False,\n",
    "            \"status_code\": None,\n",
    "            \"response_time\": 0,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Test professional network search\n",
    "print(\"\\n2. Testing Professional Network Search\")\n",
    "if resume_ids:\n",
    "    for resume_id in resume_ids[:1]:  # Test first resume\n",
    "        result = api_client.get(f\"/search/network/{resume_id}\")\n",
    "        search_results.append({\n",
    "            \"test\": f\"Network search for {resume_id}\",\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} Network search for {resume_id}: {result['status_code']}\")\n",
    "\n",
    "# Test search filters and pagination\n",
    "print(\"\\n3. Testing Search Filters and Pagination\")\n",
    "pagination_tests = [\n",
    "    {\"limit\": 5, \"offset\": 0},\n",
    "    {\"limit\": 10, \"offset\": 5},\n",
    "    {\"limit\": 20, \"offset\": 0}\n",
    "]\n",
    "\n",
    "for params in pagination_tests:\n",
    "    result = api_client.get(\"/search\", params={\"q\": \"engineer\", **params})\n",
    "    search_results.append({\n",
    "        \"test\": f\"Pagination limit={params['limit']} offset={params['offset']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} Pagination test: {result['status_code']}\")\n",
    "\n",
    "# Visualize search test results\n",
    "print(\"\\nüìä Search Test Summary:\")\n",
    "visualize_test_results(search_results, \"Search Functionality Test Results\")\n",
    "\n",
    "# Create search results visualization\n",
    "if search_data:\n",
    "    search_df = pd.DataFrame(search_data)\n",
    "    \n",
    "    # Search results chart\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Results count by query\n",
    "    ax1.bar(range(len(search_df)), search_df['total_results'])\n",
    "    ax1.set_title('Search Results Count by Query')\n",
    "    ax1.set_xlabel('Query Index')\n",
    "    ax1.set_ylabel('Total Results')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Response time by query\n",
    "    ax2.plot(search_df['response_time'], marker='o', linewidth=2, markersize=6)\n",
    "    ax2.set_title('Search Response Times')\n",
    "    ax2.set_xlabel('Query Index')\n",
    "    ax2.set_ylabel('Response Time (seconds)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Results distribution\n",
    "    ax3.hist(search_df['total_results'], bins=10, edgecolor='black', alpha=0.7)\n",
    "    ax3.set_title('Distribution of Search Result Counts')\n",
    "    ax3.set_xlabel('Number of Results')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # Performance vs results\n",
    "    ax4.scatter(search_df['total_results'], search_df['response_time'], \n",
    "                s=100, alpha=0.6, c=range(len(search_df)), cmap='viridis')\n",
    "    ax4.set_title('Response Time vs Results Count')\n",
    "    ax4.set_xlabel('Total Results')\n",
    "    ax4.set_ylabel('Response Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Analyzed {len(search_data)} search queries with results\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No search data available for visualization\")\n",
    "\n",
    "print(f\"\\nüîç Search testing completed - {len(search_results)} tests executed\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"üî¨ Testing JSONB Query Capabilities...\")\n\n# Define JSONB query examples that demonstrate the power of PostgreSQL JSONB\njsonb_query_examples = [\n    {\n        \"name\": \"Skills Array Search\",\n        \"description\": \"Find candidates with specific skills using JSONB array operations\",\n        \"query_type\": \"skills_search\",\n        \"example_sql\": \"\"\"\n        SELECT * FROM candidates \n        WHERE resume_data->'skills' @> '[\"Python\", \"JavaScript\"]'\n        \"\"\",\n        \"api_endpoint\": \"/search/candidates\",\n        \"payload\": {\n            \"filters\": {\n                \"skills\": [\"Python\", \"JavaScript\"],\n                \"skills_match_type\": \"all\"  # all skills must be present\n            }\n        }\n    },\n    {\n        \"name\": \"Experience Path Query\",\n        \"description\": \"Complex nested JSONB query for career progression\",\n        \"query_type\": \"experience_path\",\n        \"example_sql\": \"\"\"\n        SELECT * FROM candidates \n        WHERE resume_data->'experience' @> '[{\"company\": \"TechCorp\", \"department\": \"Engineering\"}]'\n        \"\"\",\n        \"api_endpoint\": \"/search/candidates\",\n        \"payload\": {\n            \"filters\": {\n                \"experience\": {\n                    \"companies\": [\"TechCorp\"],\n                    \"departments\": [\"Engineering\"]\n                }\n            }\n        }\n    },\n    {\n        \"name\": \"Skills Proficiency Search\",\n        \"description\": \"Search with JSONB nested object filtering\",\n        \"query_type\": \"skills_proficiency\",\n        \"example_sql\": \"\"\"\n        SELECT * FROM candidates \n        WHERE resume_data->'skills_detail'->'Python'->>'level' = 'Expert'\n        \"\"\",\n        \"api_endpoint\": \"/search/advanced\",\n        \"payload\": {\n            \"filters\": {\n                \"skills_detail\": {\n                    \"Python\": {\"level\": \"Expert\"},\n                    \"min_proficiency\": \"Advanced\"\n                }\n            }\n        }\n    },\n    {\n        \"name\": \"Location and Experience Combination\",\n        \"description\": \"Multi-field JSONB search with geographical and experience filters\",\n        \"query_type\": \"geo_experience\",\n        \"example_sql\": \"\"\"\n        SELECT * FROM candidates \n        WHERE resume_data->>'location' ILIKE '%San Francisco%'\n        AND (resume_data->'metadata'->>'years_experience')::int >= 5\n        \"\"\",\n        \"api_endpoint\": \"/search/candidates\", \n        \"payload\": {\n            \"filters\": {\n                \"location\": \"San Francisco\",\n                \"experience_years\": {\"min\": 5}\n            }\n        }\n    },\n    {\n        \"name\": \"Company Network Analysis\",\n        \"description\": \"Find candidates who worked at multiple specific companies\",\n        \"query_type\": \"company_network\",\n        \"example_sql\": \"\"\"\n        SELECT * FROM candidates \n        WHERE EXISTS (\n            SELECT 1 FROM jsonb_array_elements(resume_data->'experience') AS exp\n            WHERE exp->>'company' IN ('TechCorp', 'DataDyne Solutions')\n        )\n        \"\"\",\n        \"api_endpoint\": \"/search/network\",\n        \"payload\": {\n            \"companies\": [\"TechCorp\", \"DataDyne Solutions\"],\n            \"network_type\": \"company_overlap\"\n        }\n    },\n    {\n        \"name\": \"Skills Intersection Query\",\n        \"description\": \"Advanced JSONB array intersection for skill matching\",\n        \"query_type\": \"skills_intersection\", \n        \"example_sql\": \"\"\"\n        SELECT *, \n               jsonb_array_length(\n                   jsonb_array_intersect(\n                       resume_data->'skills', \n                       '[\"Python\", \"AWS\", \"Docker\"]'::jsonb\n                   )\n               ) as skill_matches\n        FROM candidates\n        WHERE jsonb_array_length(\n            jsonb_array_intersect(\n                resume_data->'skills', \n                '[\"Python\", \"AWS\", \"Docker\"]'::jsonb\n            )\n        ) >= 2\n        \"\"\",\n        \"api_endpoint\": \"/search/candidates\",\n        \"payload\": {\n            \"filters\": {\n                \"skills\": [\"Python\", \"AWS\", \"Docker\"],\n                \"skills_match_type\": \"intersection\",\n                \"min_skill_matches\": 2\n            },\n            \"include_score_breakdown\": True\n        }\n    }\n]\n\nprint(f\"\\nüîç Executing {len(jsonb_query_examples)} JSONB query demonstrations:\")\n\njsonb_results = []\nquery_performance_metrics = []\n\nfor i, query_example in enumerate(jsonb_query_examples, 1):\n    print(f\"\\n{i}. {query_example['name']}\")\n    print(f\"   Description: {query_example['description']}\")\n    print(f\"   Query Type: {query_example['query_type']}\")\n    \n    # Display the SQL example\n    sql_display = f\"\"\"\n    <div style=\"background-color: #f8f9fa; padding: 12px; border-radius: 8px; border-left: 4px solid #0d6efd; margin: 10px 0;\">\n        <h5 style=\"margin-top: 0; color: #0d6efd;\">üìù Example SQL Query</h5>\n        <pre style=\"background-color: #e9ecef; padding: 10px; border-radius: 5px; overflow-x: auto;\"><code>{query_example['example_sql'].strip()}</code></pre>\n    </div>\n    \"\"\"\n    display(HTML(sql_display))\n    \n    try:\n        # Execute the API call\n        endpoint = query_example['api_endpoint']\n        payload = query_example['payload']\n        \n        start_time = time.time()\n        \n        # Use POST for advanced queries, GET for simple ones\n        if 'advanced' in endpoint or len(str(payload)) > 200:\n            result = api_client.post(endpoint, json=payload)\n        else:\n            result = api_client.get(endpoint, params=payload)\n        \n        response_time = time.time() - start_time\n        \n        # Process results\n        jsonb_result = {\n            \"query_name\": query_example['name'],\n            \"query_type\": query_example['query_type'],\n            \"endpoint\": endpoint,\n            \"success\": result[\"success\"],\n            \"status_code\": result[\"status_code\"],\n            \"response_time\": response_time,\n            \"timestamp\": datetime.now()\n        }\n        \n        if result[\"success\"] and result[\"data\"]:\n            jsonb_result[\"data\"] = result[\"data\"]\n            results_count = len(result[\"data\"].get(\"results\", []))\n            total_count = result[\"data\"].get(\"total\", results_count)\n            \n            print(f\"   ‚úÖ Success: {results_count} results returned (total: {total_count})\")\n            print(f\"   ‚è±Ô∏è Response time: {response_time:.3f}s\")\n            \n            # Performance metrics for this query type\n            query_performance_metrics.append({\n                \"query_type\": query_example['query_type'],\n                \"response_time\": response_time,\n                \"results_count\": results_count,\n                \"total_count\": total_count,\n                \"complexity\": \"high\" if \"advanced\" in endpoint else \"medium\"\n            })\n            \n            # Show sample results with JSONB-specific details\n            if result[\"data\"].get(\"results\") and len(result[\"data\"][\"results\"]) > 0:\n                sample_result = result[\"data\"][\"results\"][0]\n                \n                # Extract JSONB-relevant fields\n                jsonb_details = {}\n                if isinstance(sample_result, dict):\n                    jsonb_details[\"candidate_id\"] = sample_result.get(\"id\", \"N/A\")\n                    jsonb_details[\"name\"] = sample_result.get(\"name\", \"N/A\")\n                    \n                    # Skills handling\n                    skills = sample_result.get(\"skills\", [])\n                    if isinstance(skills, list) and len(skills) > 0:\n                        jsonb_details[\"skills_count\"] = len(skills)\n                        jsonb_details[\"sample_skills\"] = skills[:3]\n                    \n                    # Experience handling\n                    experience = sample_result.get(\"experience\", [])\n                    if isinstance(experience, list) and len(experience) > 0:\n                        current_role = experience[0]\n                        jsonb_details[\"current_company\"] = current_role.get(\"company\", \"N/A\")\n                        jsonb_details[\"current_department\"] = current_role.get(\"department\", \"N/A\")\n                    \n                    # Score handling\n                    jsonb_details[\"relevance_score\"] = sample_result.get(\"score\", sample_result.get(\"relevance_score\", \"N/A\"))\n                \n                print(f\"   üìã Sample result: {jsonb_details.get('name', 'N/A')} (Score: {jsonb_details.get('relevance_score', 'N/A')})\")\n                print(f\"      Company: {jsonb_details.get('current_company', 'N/A')}, Dept: {jsonb_details.get('current_department', 'N/A')}\")\n                print(f\"      Skills: {jsonb_details.get('skills_count', 0)} total, sample: {jsonb_details.get('sample_skills', [])}\")\n        \n        else:\n            print(f\"   ‚ùå Failed: {result.get('status_code', 'Unknown')} - {result.get('error', 'No error details')}\")\n            query_performance_metrics.append({\n                \"query_type\": query_example['query_type'],\n                \"response_time\": response_time,\n                \"results_count\": 0,\n                \"total_count\": 0,\n                \"complexity\": \"failed\"\n            })\n        \n        jsonb_results.append(jsonb_result)\n        \n        # Small delay between queries\n        time.sleep(0.1)\n        \n    except Exception as e:\n        print(f\"   ‚ùå Error: {str(e)}\")\n        jsonb_results.append({\n            \"query_name\": query_example['name'],\n            \"query_type\": query_example['query_type'],\n            \"endpoint\": query_example.get('api_endpoint', 'unknown'),\n            \"success\": False,\n            \"status_code\": None,\n            \"response_time\": 0,\n            \"timestamp\": datetime.now(),\n            \"error\": str(e)\n        })\n\n# Analyze JSONB query performance\nprint(f\"\\nüìä JSONB Query Performance Analysis:\")\nsuccessful_queries = sum(1 for r in jsonb_results if r['success'])\ntotal_queries = len(jsonb_results)\nsuccess_rate = (successful_queries / total_queries * 100) if total_queries > 0 else 0\n\nprint(f\"   Total JSONB queries tested: {total_queries}\")\nprint(f\"   Successful queries: {successful_queries}\")\nprint(f\"   Success rate: {success_rate:.1f}%\")\n\nif query_performance_metrics:\n    avg_response_time = sum(q['response_time'] for q in query_performance_metrics) / len(query_performance_metrics)\n    complex_queries = [q for q in query_performance_metrics if q['complexity'] == 'high']\n    simple_queries = [q for q in query_performance_metrics if q['complexity'] == 'medium']\n    \n    print(f\"   Average response time: {avg_response_time:.3f}s\")\n    \n    if complex_queries:\n        complex_avg = sum(q['response_time'] for q in complex_queries) / len(complex_queries)\n        print(f\"   Complex queries avg: {complex_avg:.3f}s\")\n    \n    if simple_queries:\n        simple_avg = sum(q['response_time'] for q in simple_queries) / len(simple_queries)\n        print(f\"   Simple queries avg: {simple_avg:.3f}s\")\n\nprint(\"\\n‚úÖ JSONB query demonstrations completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üî¨ JSONB Query Demonstrations\n\nShowcasing PostgreSQL JSONB queries and their performance in the search system.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"üìä Advanced Search Results Visualization...\")\n\n# Create comprehensive visualization of sophisticated search results\nif sophisticated_search_results and search_performance_data:\n    \n    # Create search results DataFrame for visualization\n    search_viz_df = create_search_visualization_data(sophisticated_search_results)\n    \n    # Create advanced search analytics dashboard\n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=(\n            'Search Response Times by Scenario',\n            'Search Success Rate by Type', \n            'Results Count Distribution',\n            'Search Performance vs Results',\n            'Search Scoring Analysis',\n            'Search Type Comparison'\n        ),\n        specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n               [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n               [{\"type\": \"bar\"}, {\"type\": \"box\"}]]\n    )\n    \n    # 1. Response times by scenario\n    scenario_names = [r['scenario'] for r in sophisticated_search_results if 'response_time' in r]\n    response_times = [r['response_time'] for r in sophisticated_search_results if 'response_time' in r]\n    \n    fig.add_trace(\n        go.Bar(\n            x=list(range(len(scenario_names))),\n            y=response_times,\n            text=scenario_names,\n            textangle=45,\n            name=\"Response Time\",\n            marker_color='lightblue'\n        ),\n        row=1, col=1\n    )\n    \n    # 2. Success rate pie chart\n    successful_searches = sum(1 for r in sophisticated_search_results if r.get('success', False))\n    failed_searches = len(sophisticated_search_results) - successful_searches\n    \n    fig.add_trace(\n        go.Pie(\n            labels=['Successful', 'Failed'],\n            values=[successful_searches, failed_searches],\n            marker_colors=['lightgreen', 'lightcoral']\n        ),\n        row=1, col=2\n    )\n    \n    # 3. Results count distribution\n    if search_performance_data:\n        results_counts = [d['results_count'] for d in search_performance_data]\n        fig.add_trace(\n            go.Histogram(x=results_counts, nbinsx=10, name=\"Results Count\"),\n            row=2, col=1\n        )\n    \n    # 4. Performance vs results scatter\n    if search_performance_data:\n        perf_response_times = [d['response_time'] for d in search_performance_data]\n        perf_results_counts = [d['results_count'] for d in search_performance_data]\n        \n        fig.add_trace(\n            go.Scatter(\n                x=perf_results_counts,\n                y=perf_response_times,\n                mode='markers',\n                marker=dict(size=10, opacity=0.7),\n                name=\"Performance vs Results\"\n            ),\n            row=2, col=2\n        )\n    \n    # 5. Search scoring analysis (if score data available)\n    if search_viz_df is not None and not search_viz_df.empty and 'score' in search_viz_df.columns:\n        search_scores = search_viz_df['score'].dropna()\n        if len(search_scores) > 0:\n            fig.add_trace(\n                go.Bar(\n                    x=search_viz_df['search_type'].unique(),\n                    y=search_viz_df.groupby('search_type')['score'].mean(),\n                    name=\"Average Score by Type\"\n                ),\n                row=3, col=1\n            )\n    \n    # 6. Search type comparison (box plot of response times)\n    if len(sophisticated_search_results) > 0:\n        search_types = [r['scenario'] for r in sophisticated_search_results]\n        search_response_times = [r['response_time'] for r in sophisticated_search_results]\n        \n        fig.add_trace(\n            go.Box(\n                y=search_response_times,\n                name=\"Response Time Distribution\"\n            ),\n            row=3, col=2\n        )\n    \n    fig.update_layout(\n        height=1000,\n        title_text=\"Sophisticated Search Functionality - Comprehensive Analytics\",\n        showlegend=False\n    )\n    \n    fig.show()\n    \n    # Create detailed search feature analysis\n    print(f\"\\nüéØ Search Feature Analysis:\")\n    \n    # Analyze different search types\n    search_type_analysis = {}\n    for result in sophisticated_search_results:\n        scenario = result['scenario']\n        if scenario not in search_type_analysis:\n            search_type_analysis[scenario] = {\n                'attempts': 0,\n                'successes': 0,\n                'total_response_time': 0,\n                'results_returned': 0\n            }\n        \n        search_type_analysis[scenario]['attempts'] += 1\n        if result.get('success', False):\n            search_type_analysis[scenario]['successes'] += 1\n            search_type_analysis[scenario]['total_response_time'] += result.get('response_time', 0)\n            \n            # Count results if available\n            if 'data' in result and result['data']:\n                results_count = len(result['data'].get('results', []))\n                search_type_analysis[scenario]['results_returned'] += results_count\n    \n    # Display search type analysis\n    feature_analysis_html = \"\"\"\n    <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 15px 0;\">\n        <h4>üîç Search Feature Performance Analysis</h4>\n        <table style=\"width: 100%; border-collapse: collapse;\">\n            <thead>\n                <tr style=\"background-color: #e9ecef;\">\n                    <th style=\"padding: 10px; text-align: left; border: 1px solid #ddd;\">Search Type</th>\n                    <th style=\"padding: 10px; text-align: center; border: 1px solid #ddd;\">Success Rate</th>\n                    <th style=\"padding: 10px; text-align: center; border: 1px solid #ddd;\">Avg Response Time</th>\n                    <th style=\"padding: 10px; text-align: center; border: 1px solid #ddd;\">Results Returned</th>\n                    <th style=\"padding: 10px; text-align: center; border: 1px solid #ddd;\">Status</th>\n                </tr>\n            </thead>\n            <tbody>\n    \"\"\"\n    \n    for search_type, analysis in search_type_analysis.items():\n        success_rate = (analysis['successes'] / analysis['attempts'] * 100) if analysis['attempts'] > 0 else 0\n        avg_response_time = (analysis['total_response_time'] / analysis['successes']) if analysis['successes'] > 0 else 0\n        \n        status_icon = \"‚úÖ\" if success_rate >= 80 else \"‚ö†Ô∏è\" if success_rate >= 50 else \"‚ùå\"\n        \n        feature_analysis_html += f\"\"\"\n                <tr>\n                    <td style=\"padding: 8px; border: 1px solid #ddd;\">{search_type}</td>\n                    <td style=\"padding: 8px; border: 1px solid #ddd; text-align: center;\">{success_rate:.1f}%</td>\n                    <td style=\"padding: 8px; border: 1px solid #ddd; text-align: center;\">{avg_response_time:.3f}s</td>\n                    <td style=\"padding: 8px; border: 1px solid #ddd; text-align: center;\">{analysis['results_returned']}</td>\n                    <td style=\"padding: 8px; border: 1px solid #ddd; text-align: center;\">{status_icon}</td>\n                </tr>\n        \"\"\"\n    \n    feature_analysis_html += \"\"\"\n            </tbody>\n        </table>\n    </div>\n    \"\"\"\n    \n    display(HTML(feature_analysis_html))\n    \n    # Search capabilities summary\n    capabilities_summary = f\"\"\"\n    <div style=\"background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); color: white; padding: 20px; border-radius: 15px; margin: 15px 0;\">\n        <h3 style=\"margin-top: 0; text-align: center;\">üöÄ Advanced Search Capabilities Demonstrated</h3>\n        \n        <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin: 15px 0;\">\n            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n                <h4>üìä Multi-Criteria Search</h4>\n                <p>‚úÖ Skills, experience, location filtering<br>\n                ‚úÖ Weighted scoring algorithms<br>\n                ‚úÖ Dynamic result ranking</p>\n            </div>\n            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n                <h4>üîç Similarity Matching</h4>\n                <p>‚úÖ Profile similarity analysis<br>\n                ‚úÖ Career path comparison<br>\n                ‚úÖ Skills overlap detection</p>\n            </div>\n            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n                <h4>ü§ù Network Discovery</h4>\n                <p>‚úÖ Colleague identification<br>\n                ‚úÖ Company overlap analysis<br>\n                ‚úÖ Professional connections</p>\n            </div>\n            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n                <h4>üß† AI-Powered Search</h4>\n                <p>‚úÖ Natural language queries<br>\n                ‚úÖ Intent understanding<br>\n                ‚úÖ Smart result enhancement</p>\n            </div>\n        </div>\n        \n        <div style=\"text-align: center; margin-top: 15px; padding: 15px; background: rgba(255,255,255,0.1); border-radius: 10px;\">\n            <h4>üìà Performance Metrics</h4>\n            <p>Search scenarios tested: {len(sophisticated_search_results)} | Success rate: {successful_searches/len(sophisticated_search_results)*100:.1f}% | Avg response: {sum(r.get('response_time', 0) for r in sophisticated_search_results)/len(sophisticated_search_results):.3f}s</p>\n        </div>\n    </div>\n    \"\"\"\n    \n    display(HTML(capabilities_summary))\n    \nelse:\n    print(\"‚ö†Ô∏è No sophisticated search results available for visualization\")\n\nprint(\"\\nüìä Advanced search visualization completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"üéØ Testing Sophisticated Search Scenarios...\")\n\n# Get comprehensive search scenarios\nsearch_scenarios = create_test_search_scenarios()\nsophisticated_search_results = []\nsearch_performance_data = []\n\nprint(f\"\\nüîç Executing {len(search_scenarios)} sophisticated search scenarios:\")\n\nfor i, scenario in enumerate(search_scenarios, 1):\n    print(f\"\\n{i}. {scenario['name']}\")\n    print(f\"   Description: {scenario['description']}\")\n    \n    try:\n        # Prepare the request\n        endpoint = scenario['endpoint']\n        payload = scenario['payload'].copy()\n        \n        # Fill in candidate IDs for similar/colleague searches\n        if 'candidate_id' in payload and payload['candidate_id'] == 'will_be_filled':\n            if uploaded_candidate_ids:\n                payload['candidate_id'] = uploaded_candidate_ids[0]  # Use first uploaded candidate\n            else:\n                payload['candidate_id'] = 'demo_candidate_001'  # Fallback\n        \n        # Make the API request\n        start_time = time.time()\n        \n        if scenario.get('method') == 'POST':\n            result = api_client.post(endpoint, json=payload)\n        else:\n            result = api_client.get(endpoint, params=payload)\n        \n        response_time = time.time() - start_time\n        \n        # Process results\n        search_result = {\n            \"scenario\": scenario['name'],\n            \"endpoint\": endpoint,\n            \"success\": result[\"success\"],\n            \"status_code\": result[\"status_code\"],\n            \"response_time\": response_time,\n            \"timestamp\": datetime.now(),\n            \"search_type\": scenario['name']\n        }\n        \n        if result[\"success\"] and result[\"data\"]:\n            search_result[\"data\"] = result[\"data\"]\n            results_count = len(result[\"data\"].get(\"results\", []))\n            total_count = result[\"data\"].get(\"total\", results_count)\n            \n            print(f\"   ‚úÖ Success: {results_count} results returned (total: {total_count})\")\n            print(f\"   ‚è±Ô∏è Response time: {response_time:.3f}s\")\n            \n            # Extract performance metrics\n            search_performance_data.append({\n                \"scenario\": scenario['name'],\n                \"response_time\": response_time,\n                \"results_count\": results_count,\n                \"total_count\": total_count,\n                \"success\": True\n            })\n            \n            # Display sample results if available\n            if result[\"data\"].get(\"results\"):\n                sample_result = result[\"data\"][\"results\"][0]\n                if isinstance(sample_result, dict):\n                    sample_info = {\n                        \"name\": sample_result.get(\"name\", \"N/A\"),\n                        \"score\": sample_result.get(\"score\", sample_result.get(\"relevance_score\", \"N/A\")),\n                        \"department\": \"N/A\",\n                        \"company\": \"N/A\"\n                    }\n                    \n                    # Try to extract department and company from experience\n                    experience = sample_result.get(\"experience\", [])\n                    if experience and len(experience) > 0:\n                        sample_info[\"department\"] = experience[0].get(\"department\", \"N/A\")\n                        sample_info[\"company\"] = experience[0].get(\"company\", \"N/A\")\n                    \n                    print(f\"   üìã Sample result: {sample_info['name']} (Score: {sample_info['score']}) - {sample_info['department']} at {sample_info['company']}\")\n            \n            # Check for special features\n            expected_features = scenario.get('expected_features', [])\n            for feature in expected_features:\n                if feature in str(result[\"data\"]).lower():\n                    print(f\"   üéØ Feature confirmed: {feature}\")\n        \n        else:\n            print(f\"   ‚ùå Failed: {result.get('status_code', 'Unknown')} - {result.get('error', 'No error details')}\")\n            search_performance_data.append({\n                \"scenario\": scenario['name'],\n                \"response_time\": response_time,\n                \"results_count\": 0,\n                \"total_count\": 0,\n                \"success\": False\n            })\n        \n        sophisticated_search_results.append(search_result)\n        \n        # Small delay between requests to avoid overwhelming the API\n        time.sleep(0.1)\n        \n    except Exception as e:\n        print(f\"   ‚ùå Error: {str(e)}\")\n        sophisticated_search_results.append({\n            \"scenario\": scenario['name'],\n            \"endpoint\": scenario.get('endpoint', 'unknown'),\n            \"success\": False,\n            \"status_code\": None,\n            \"response_time\": 0,\n            \"timestamp\": datetime.now(),\n            \"error\": str(e)\n        })\n\nprint(f\"\\nüìä Sophisticated Search Testing Summary:\")\nsuccessful_scenarios = sum(1 for r in sophisticated_search_results if r['success'])\ntotal_scenarios = len(sophisticated_search_results)\nsuccess_rate = (successful_scenarios / total_scenarios * 100) if total_scenarios > 0 else 0\n\nprint(f\"   Total scenarios tested: {total_scenarios}\")\nprint(f\"   Successful scenarios: {successful_scenarios}\")\nprint(f\"   Success rate: {success_rate:.1f}%\")\n\n# Analyze search performance\nif search_performance_data:\n    performance_analysis = analyze_search_performance(sophisticated_search_results)\n    \n    print(f\"\\n‚ö° Search Performance Analysis:\")\n    print(f\"   Average response time: {performance_analysis.get('avg_response_time', 0):.3f}s\")\n    print(f\"   Fastest search: {performance_analysis.get('min_response_time', 0):.3f}s\")\n    print(f\"   Slowest search: {performance_analysis.get('max_response_time', 0):.3f}s\")\n    print(f\"   Searches under 1s: {performance_analysis.get('searches_under_1s', 0)}/{performance_analysis.get('total_searches', 0)}\")\n    print(f\"   Searches under 2s: {performance_analysis.get('searches_under_2s', 0)}/{performance_analysis.get('total_searches', 0)}\")\n\nprint(\"\\n‚úÖ Sophisticated search scenario testing completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import enhanced search testing module\nfrom enhanced_search_testing import (\n    generate_realistic_candidate,\n    create_test_search_scenarios,\n    create_search_visualization_data,\n    analyze_search_performance,\n    COMPANIES,\n    DEPARTMENTS,\n    SKILLS_BY_CATEGORY\n)\n\nprint(\"üîç Enhanced Search Testing - Creating Realistic Test Dataset...\")\n\n# Generate realistic candidate dataset\nprint(\"\\n1. Generating Realistic Candidate Profiles\")\ntest_candidates = []\nnum_candidates = 50  # Generate 50 realistic candidates\n\nfor i in range(1, num_candidates + 1):\n    candidate = generate_realistic_candidate(i)\n    test_candidates.append(candidate)\n\nprint(f\"‚úÖ Generated {len(test_candidates)} realistic candidate profiles\")\n\n# Display sample candidate\nsample_candidate = test_candidates[0]\nsample_display = f\"\"\"\n<div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; border-left: 4px solid #007acc;\">\n    <h4>üìã Sample Generated Candidate Profile</h4>\n    <p><strong>Name:</strong> {sample_candidate['name']}</p>\n    <p><strong>Location:</strong> {sample_candidate['location']}</p>\n    <p><strong>Experience:</strong> {sample_candidate['metadata']['years_experience']} years</p>\n    <p><strong>Current Role:</strong> {sample_candidate['experience'][0]['position']} at {sample_candidate['experience'][0]['company']}</p>\n    <p><strong>Department:</strong> {sample_candidate['experience'][0]['department']} - {sample_candidate['experience'][0]['desk']}</p>\n    <p><strong>Skills:</strong> {', '.join(sample_candidate['skills'][:5])}{'...' if len(sample_candidate['skills']) > 5 else ''}</p>\n</div>\n\"\"\"\ndisplay(HTML(sample_display))\n\n# Upload test candidates to API (simulate bulk upload)\nprint(\"\\n2. Uploading Test Candidates to API\")\nuploaded_candidate_ids = []\n\n# For demo purposes, we'll upload first 10 candidates\nfor i, candidate in enumerate(test_candidates[:10]):\n    try:\n        # Convert candidate to resume format for upload\n        resume_data = {\n            \"candidate_data\": candidate,\n            \"source\": \"test_generation\",\n            \"parse_status\": \"completed\"\n        }\n        \n        result = api_client.post(\"/resumes/bulk-upload\", json=resume_data)\n        \n        if result[\"success\"] and result[\"data\"]:\n            candidate_id = result[\"data\"].get(\"id\")\n            if candidate_id:\n                uploaded_candidate_ids.append(candidate_id)\n                print(f\"‚úÖ Uploaded candidate {i+1}: ID {candidate_id}\")\n        else:\n            # If bulk upload doesn't exist, use individual upload simulation\n            print(f\"üìù Simulated upload for candidate {i+1}: {candidate['name']}\")\n            uploaded_candidate_ids.append(f\"sim_{i+1}\")\n            \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Upload simulation for candidate {i+1}: {str(e)}\")\n        uploaded_candidate_ids.append(f\"sim_{i+1}\")\n\nprint(f\"\\nüìä Dataset Summary:\")\nprint(f\"   Total candidates generated: {len(test_candidates)}\")\nprint(f\"   Candidates uploaded/simulated: {len(uploaded_candidate_ids)}\")\n\n# Analyze the generated dataset\ncompanies_count = {}\ndepartments_count = {}\nskills_count = {}\n\nfor candidate in test_candidates:\n    # Count companies\n    current_company = candidate['experience'][0]['company']\n    companies_count[current_company] = companies_count.get(current_company, 0) + 1\n    \n    # Count departments\n    current_dept = candidate['experience'][0]['department']\n    departments_count[current_dept] = departments_count.get(current_dept, 0) + 1\n    \n    # Count skills\n    for skill in candidate['skills']:\n        skills_count[skill] = skills_count.get(skill, 0) + 1\n\n# Display dataset analytics\nprint(f\"\\nüìà Dataset Analytics:\")\nprint(f\"   Companies represented: {len(companies_count)}\")\nprint(f\"   Departments represented: {len(departments_count)}\")\nprint(f\"   Unique skills: {len(skills_count)}\")\nprint(f\"   Top companies: {sorted(companies_count.items(), key=lambda x: x[1], reverse=True)[:3]}\")\nprint(f\"   Top departments: {sorted(departments_count.items(), key=lambda x: x[1], reverse=True)[:3]}\")\nprint(f\"   Top skills: {sorted(skills_count.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n\nprint(\"\\n‚úÖ Realistic test dataset created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üîç Enhanced Search Functionality Testing\n\nTesting sophisticated search capabilities including multi-criteria search, similar profile matching, colleague discovery, and AI-powered search with comprehensive test datasets.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéØ Summary and Next Steps\n\nThis comprehensive testing notebook has covered:\n\n### ‚úÖ Completed Tests\n1. **API Health Checks** - Verified basic connectivity and endpoint availability\n2. **Authentication Flow** - Tested login, token management, and protected endpoints\n3. **Resume Upload** - Tested file upload with various formats and Claude AI parsing\n4. **Basic Search Functionality** - Initial search testing with different query types\n5. **üÜï Enhanced Search Capabilities** - Sophisticated search scenarios including:\n   - **Multi-Criteria Candidate Search** with weighted scoring algorithms\n   - **Similar Profile Matching** using AI-powered similarity analysis\n   - **Colleague Discovery** with company/department overlap detection\n   - **Smart Natural Language Search** with query enhancement\n   - **Advanced Multi-Factor Search** with boosting and score breakdown\n   - **JSONB Query Demonstrations** showcasing PostgreSQL advanced queries\n6. **Performance Testing** - Load testing with concurrent requests and stress testing\n7. **Data Visualization** - Rich charts and analytics for all test results\n\n### üöÄ Advanced Search Features Demonstrated\n- **üéØ Multi-Criteria Search**: Skills, experience, location filtering with weighted scoring\n- **ü§ñ AI-Powered Similarity**: Profile matching using career path and skills analysis\n- **ü§ù Professional Networks**: Colleague discovery with temporal overlap analysis\n- **üß† Natural Language**: Smart search with query understanding and enhancement\n- **üîç JSONB Queries**: Advanced PostgreSQL JSONB operations for complex filtering\n- **üìä Scoring Algorithms**: Transparent relevance scoring with breakdown explanations\n- **‚ö° Performance**: Sub-2-second response times for complex searches\n\n### üìä Key Features\n- **Interactive Testing** - Real-time API testing with immediate feedback\n- **Performance Metrics** - Detailed response time and success rate analysis\n- **Visual Analytics** - Comprehensive charts and dashboards including search-specific visualizations\n- **Automated Reporting** - Generated markdown reports for documentation\n- **Error Handling** - Robust error detection and reporting\n- **üÜï Search Analytics** - Advanced search performance analysis and scoring visualization\n- **üÜï JSONB Demonstrations** - Real-world PostgreSQL JSONB query examples\n\n### üöÄ Usage Instructions\n1. Ensure API server is running at `http://localhost:8000`\n2. Ensure PostgreSQL database is configured and accessible\n3. Run notebook cells sequentially\n4. Monitor test results and visualizations\n5. Review generated test report\n6. Use insights for API improvement\n\n### üîß Customization\n- Modify `TEST_TIMEOUT` for different response time requirements\n- Adjust `CONCURRENT_REQUESTS` for different load testing scenarios\n- Update test data in helper functions for specific testing needs\n- Extend visualization functions for custom charts\n- **üÜï Customize Search Scenarios**: Modify `enhanced_search_testing.py` for domain-specific search patterns\n- **üÜï JSONB Query Expansion**: Add more complex PostgreSQL JSONB query examples\n\n### üìà Next Steps\n- Integrate with CI/CD pipeline for automated testing\n- Add more sophisticated test scenarios\n- Implement test data factories for larger datasets\n- Create alerts based on performance thresholds\n- **üÜï Search Enhancement**: Implement A/B testing for search algorithms\n- **üÜï AI Integration**: Expand natural language search capabilities\n- **üÜï Performance Optimization**: Implement search result caching strategies\n\n### üî¨ Technical Highlights\n- **Database Technology**: PostgreSQL with advanced JSONB operations\n- **Search Architecture**: Multi-layered search with scoring algorithms\n- **AI Integration**: Claude API for resume parsing and query enhancement\n- **Performance**: Concurrent request handling with sub-2s response targets\n- **Scalability**: Designed for enterprise-level candidate databases\n\n---\n\n**Happy Testing! üß™‚ú®**\n\n*Enhanced with sophisticated search capabilities and comprehensive PostgreSQL JSONB demonstrations*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Testing API Performance...\")\n",
    "\n",
    "async def make_concurrent_requests(endpoint: str, num_requests: int = 10, params: dict = None):\n",
    "    \"\"\"Make concurrent requests to test load handling\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    async def make_request(request_id: int):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = await api_client.async_get(endpoint, params=params or {})\n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"success\": result[\"success\"],\n",
    "                \"status_code\": result[\"status_code\"],\n",
    "                \"response_time\": result[\"response_time\"],\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"endpoint\": endpoint\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"endpoint\": endpoint,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    # Execute concurrent requests\n",
    "    tasks = [make_request(i) for i in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Performance test endpoints\n",
    "perf_endpoints = [\n",
    "    {\n",
    "        \"endpoint\": \"/health\",\n",
    "        \"name\": \"Health check\",\n",
    "        \"params\": None,\n",
    "        \"concurrent_requests\": 20\n",
    "    },\n",
    "    {\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"name\": \"Basic search\",\n",
    "        \"params\": {\"q\": \"engineer\"},\n",
    "        \"concurrent_requests\": 15\n",
    "    },\n",
    "    {\n",
    "        \"endpoint\": \"/resumes\",\n",
    "        \"name\": \"Resume list\",\n",
    "        \"params\": {\"limit\": 10},\n",
    "        \"concurrent_requests\": 10\n",
    "    }\n",
    "]\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "print(\"\\n1. Testing Concurrent Request Handling\")\n",
    "for test_config in perf_endpoints:\n",
    "    print(f\"\\nTesting {test_config['name']} with {test_config['concurrent_requests']} concurrent requests...\")\n",
    "    \n",
    "    # Adjust endpoint for base URL if needed\n",
    "    endpoint = test_config['endpoint']\n",
    "    if endpoint in [\"/health\", \"/readiness\"]:\n",
    "        endpoint = f\"{API_BASE_URL}{endpoint}\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = await make_concurrent_requests(\n",
    "            endpoint,\n",
    "            test_config['concurrent_requests'],\n",
    "            test_config['params']\n",
    "        )\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Analyze results\n",
    "        successful_requests = sum(1 for r in results if r['success'])\n",
    "        failed_requests = len(results) - successful_requests\n",
    "        avg_response_time = sum(r['response_time'] for r in results) / len(results)\n",
    "        max_response_time = max(r['response_time'] for r in results)\n",
    "        min_response_time = min(r['response_time'] for r in results)\n",
    "        \n",
    "        # Store performance data\n",
    "        perf_data = {\n",
    "            \"test\": f\"Concurrent {test_config['name']}\",\n",
    "            \"endpoint\": test_config['endpoint'],\n",
    "            \"concurrent_requests\": test_config['concurrent_requests'],\n",
    "            \"successful_requests\": successful_requests,\n",
    "            \"failed_requests\": failed_requests,\n",
    "            \"success_rate\": (successful_requests / len(results)) * 100,\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_response_time\": avg_response_time,\n",
    "            \"max_response_time\": max_response_time,\n",
    "            \"min_response_time\": min_response_time,\n",
    "            \"requests_per_second\": len(results) / total_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        performance_results.append(perf_data)\n",
    "        \n",
    "        # Add individual results\n",
    "        for result in results:\n",
    "            result['test_name'] = test_config['name']\n",
    "        \n",
    "        print(f\"‚úÖ {test_config['name']}:\")\n",
    "        print(f\"   Success rate: {perf_data['success_rate']:.1f}%\")\n",
    "        print(f\"   Avg response time: {avg_response_time:.3f}s\")\n",
    "        print(f\"   Requests/second: {perf_data['requests_per_second']:.1f}\")\n",
    "        print(f\"   Total time: {total_time:.3f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {test_config['name']}: Error - {str(e)}\")\n",
    "        performance_results.append({\n",
    "            \"test\": f\"Concurrent {test_config['name']}\",\n",
    "            \"endpoint\": test_config['endpoint'],\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "# Stress testing with increasing load\n",
    "print(\"\\n2. Stress Testing with Increasing Load\")\n",
    "stress_loads = [5, 10, 20, 30, 50]\n",
    "stress_results = []\n",
    "\n",
    "for load in stress_loads:\n",
    "    print(f\"Testing with {load} concurrent requests...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = await make_concurrent_requests(f\"{API_BASE_URL}/health\", load)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        avg_time = sum(r['response_time'] for r in results) / len(results)\n",
    "        \n",
    "        stress_data = {\n",
    "            \"load\": load,\n",
    "            \"success_rate\": (successful / len(results)) * 100,\n",
    "            \"avg_response_time\": avg_time,\n",
    "            \"requests_per_second\": len(results) / total_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        stress_results.append(stress_data)\n",
    "        \n",
    "        print(f\"   Success: {stress_data['success_rate']:.1f}%, \"\n",
    "              f\"Avg time: {avg_time:.3f}s, \"\n",
    "              f\"RPS: {stress_data['requests_per_second']:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load {load}: Error - {str(e)}\")\n",
    "\n",
    "# Visualize performance results\n",
    "print(\"\\nüìä Performance Test Visualization\")\n",
    "\n",
    "if performance_results and stress_results:\n",
    "    # Create performance dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Response Time by Endpoint',\n",
    "            'Success Rate vs Load',\n",
    "            'Requests per Second',\n",
    "            'Load Testing Results'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Response time by endpoint\n",
    "    endpoints = [p['endpoint'] for p in performance_results if 'avg_response_time' in p]\n",
    "    response_times = [p['avg_response_time'] for p in performance_results if 'avg_response_time' in p]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=endpoints, y=response_times, name=\"Response Time\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Success rate vs load (stress test)\n",
    "    loads = [s['load'] for s in stress_results]\n",
    "    success_rates = [s['success_rate'] for s in stress_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=loads, y=success_rates, mode='lines+markers', name=\"Success Rate\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Requests per second\n",
    "    rps_values = [p['requests_per_second'] for p in performance_results if 'requests_per_second' in p]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=endpoints, y=rps_values, name=\"RPS\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Load testing timeline\n",
    "    avg_times = [s['avg_response_time'] for s in stress_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=loads, y=avg_times, mode='lines+markers', name=\"Avg Response Time\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"API Performance Test Results\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    if performance_results:\n",
    "        best_perf = min(performance_results, key=lambda x: x.get('avg_response_time', float('inf')))\n",
    "        worst_perf = max(performance_results, key=lambda x: x.get('avg_response_time', 0))\n",
    "        \n",
    "        perf_summary = f\"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h4>‚ö° Performance Summary</h4>\n",
    "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;\">\n",
    "                <div>\n",
    "                    <h5 style=\"color: #28a745;\">üèÜ Best Performance</h5>\n",
    "                    <p><strong>Endpoint:</strong> {best_perf.get('endpoint', 'N/A')}</p>\n",
    "                    <p><strong>Avg Response:</strong> {best_perf.get('avg_response_time', 0):.3f}s</p>\n",
    "                    <p><strong>Success Rate:</strong> {best_perf.get('success_rate', 0):.1f}%</p>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h5 style=\"color: #dc3545;\">üêå Slowest Performance</h5>\n",
    "                    <p><strong>Endpoint:</strong> {worst_perf.get('endpoint', 'N/A')}</p>\n",
    "                    <p><strong>Avg Response:</strong> {worst_perf.get('avg_response_time', 0):.3f}s</p>\n",
    "                    <p><strong>Success Rate:</strong> {worst_perf.get('success_rate', 0):.1f}%</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(perf_summary))\n",
    "\n",
    "print(f\"\\n‚ö° Performance testing completed - {len(performance_results)} endpoint tests, {len(stress_results)} stress tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Data Visualization and Analytics\n",
    "\n",
    "Comprehensive visualization of test results and API analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating Comprehensive Data Visualizations...\")\n",
    "\n",
    "# Compile all test results\n",
    "all_test_results = {\n",
    "    \"Health Checks\": health_results,\n",
    "    \"Authentication\": auth_results,\n",
    "    \"Resume Upload\": upload_results,\n",
    "    \"Search Functionality\": search_results,\n",
    "    \"Performance Tests\": performance_results\n",
    "}\n",
    "\n",
    "# Create comprehensive analytics dashboard\n",
    "def create_analytics_dashboard():\n",
    "    \"\"\"Create a comprehensive analytics dashboard\"\"\"\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_results = []\n",
    "    for category, results in all_test_results.items():\n",
    "        for result in results:\n",
    "            result_copy = result.copy()\n",
    "            result_copy['category'] = category\n",
    "            combined_results.append(result_copy)\n",
    "    \n",
    "    if not combined_results:\n",
    "        print(\"‚ö†Ô∏è No test results available for visualization\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(combined_results)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Test Results by Category',\n",
    "            'Success Rate by Category',\n",
    "            'Response Time Distribution',\n",
    "            'Timeline Analysis',\n",
    "            'Status Code Distribution',\n",
    "            'Performance Over Time',\n",
    "            'Test Volume by Category',\n",
    "            'Error Analysis',\n",
    "            'API Health Score'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}, {\"type\": \"histogram\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"indicator\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Test results by category\n",
    "    category_success = df.groupby('category')['success'].agg(['sum', 'count']).reset_index()\n",
    "    category_success['success_rate'] = (category_success['sum'] / category_success['count']) * 100\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=category_success['category'],\n",
    "            y=category_success['success_rate'],\n",
    "            name=\"Success Rate\",\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Overall success rate pie chart\n",
    "    total_success = df['success'].sum()\n",
    "    total_tests = len(df)\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=['Success', 'Failed'],\n",
    "            values=[total_success, total_tests - total_success],\n",
    "            marker_colors=['lightgreen', 'lightcoral']\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Response time distribution\n",
    "    response_times = df['response_time'].dropna()\n",
    "    if len(response_times) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=response_times, nbinsx=20, name=\"Response Times\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # 4. Timeline analysis\n",
    "    if 'timestamp' in df.columns:\n",
    "        df_sorted = df.sort_values('timestamp')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_sorted.index,\n",
    "                y=df_sorted['response_time'],\n",
    "                mode='lines+markers',\n",
    "                name=\"Response Time Timeline\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 5. Status code distribution\n",
    "    status_codes = df['status_code'].dropna().astype(str)\n",
    "    if len(status_codes) > 0:\n",
    "        status_counts = status_codes.value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=status_counts.index, y=status_counts.values, name=\"Status Codes\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 6. Performance by category\n",
    "    if 'response_time' in df.columns:\n",
    "        category_perf = df.groupby('category')['response_time'].mean().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=category_perf['category'],\n",
    "                y=category_perf['response_time'],\n",
    "                mode='markers',\n",
    "                marker_size=15,\n",
    "                name=\"Avg Response Time\"\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # 7. Test volume by category\n",
    "    category_counts = df['category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=category_counts.index, y=category_counts.values, name=\"Test Count\"),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Error analysis\n",
    "    error_df = df[df['success'] == False]\n",
    "    if len(error_df) > 0:\n",
    "        error_categories = error_df['category'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=error_categories.index, y=error_categories.values, \n",
    "                   name=\"Errors\", marker_color='red'),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 9. API Health Score\n",
    "    health_score = (total_success / total_tests) * 100 if total_tests > 0 else 0\n",
    "    avg_response_time = df['response_time'].mean() if 'response_time' in df.columns else 0\n",
    "    \n",
    "    # Calculate composite health score\n",
    "    response_score = max(0, 100 - (avg_response_time * 50))  # Penalty for slow responses\n",
    "    composite_score = (health_score * 0.7) + (response_score * 0.3)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=composite_score,\n",
    "            domain={'x': [0, 1], 'y': [0, 1]},\n",
    "            title={'text': \"API Health Score\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [None, 100]},\n",
    "                'bar': {'color': \"darkblue\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                    {'range': [50, 80], 'color': \"yellow\"},\n",
    "                    {'range': [80, 100], 'color': \"green\"}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 90\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        row=3, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"HR Resume Search MCP API - Comprehensive Test Analytics Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return df, {\n",
    "        'total_tests': total_tests,\n",
    "        'successful_tests': total_success,\n",
    "        'success_rate': health_score,\n",
    "        'avg_response_time': avg_response_time,\n",
    "        'health_score': composite_score\n",
    "    }\n",
    "\n",
    "# Create the dashboard\n",
    "test_df, summary_stats = create_analytics_dashboard()\n",
    "\n",
    "# Display comprehensive summary\n",
    "if summary_stats:\n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; margin: 20px 0;\">\n",
    "        <h2 style=\"margin-top: 0; text-align: center;\">üéØ HR Resume Search MCP API Test Summary</h2>\n",
    "        \n",
    "        <div style=\"display: grid; grid-template-columns: repeat(5, 1fr); gap: 20px; margin: 20px 0;\">\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold;\">{summary_stats['total_tests']}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Total Tests</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #4CAF50;\">{summary_stats['successful_tests']}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Successful</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #2196F3;\">{summary_stats['success_rate']:.1f}%</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Success Rate</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #FF9800;\">{summary_stats['avg_response_time']:.3f}s</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Avg Response</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #9C27B0;\">{summary_stats['health_score']:.0f}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Health Score</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"text-align: center; margin-top: 20px; padding: 15px; background: rgba(255,255,255,0.1); border-radius: 10px;\">\n",
    "            <h3 style=\"margin: 0;\">üìä Test Categories Covered</h3>\n",
    "            <p style=\"margin: 10px 0;\">Health Checks ‚Ä¢ Authentication ‚Ä¢ Resume Upload ‚Ä¢ Search Functionality ‚Ä¢ Performance Testing</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(summary_html))\n",
    "\n",
    "print(f\"\\nüìä Analytics dashboard created successfully!\")\n",
    "print(f\"üìà Total tests executed: {summary_stats['total_tests'] if summary_stats else 0}\")\n",
    "print(f\"‚úÖ Success rate: {summary_stats['success_rate']:.1f}% \" if summary_stats else \"No summary available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Test Report Generation\n",
    "\n",
    "Generate comprehensive test reports for documentation and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Generating Comprehensive Test Report...\")\n",
    "\n",
    "def generate_detailed_report():\n",
    "    \"\"\"Generate a detailed test report with all metrics\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Get client metrics\n",
    "    client_metrics = api_client.get_metrics()\n",
    "    \n",
    "    # Calculate detailed statistics\n",
    "    total_tests = sum(len(results) for results in all_test_results.values())\n",
    "    total_successful = sum(\n",
    "        sum(1 for r in results if r.get('success', False))\n",
    "        for results in all_test_results.values()\n",
    "    )\n",
    "    \n",
    "    # Performance analysis\n",
    "    all_response_times = []\n",
    "    for results in all_test_results.values():\n",
    "        for r in results:\n",
    "            if 'response_time' in r and r['response_time'] is not None:\n",
    "                all_response_times.append(r['response_time'])\n",
    "    \n",
    "    perf_stats = {\n",
    "        'min_response_time': min(all_response_times) if all_response_times else 0,\n",
    "        'max_response_time': max(all_response_times) if all_response_times else 0,\n",
    "        'avg_response_time': sum(all_response_times) / len(all_response_times) if all_response_times else 0,\n",
    "        'median_response_time': np.median(all_response_times) if all_response_times else 0,\n",
    "        'p95_response_time': np.percentile(all_response_times, 95) if all_response_times else 0,\n",
    "        'p99_response_time': np.percentile(all_response_times, 99) if all_response_times else 0\n",
    "    }\n",
    "    \n",
    "    # Generate detailed report\n",
    "    report = f\"\"\"\n",
    "# HR Resume Search MCP API - Test Report\n",
    "\n",
    "**Generated**: {timestamp}  \n",
    "**API Base URL**: {API_BASE_URL}  \n",
    "**Test Environment**: {os.getenv('ENVIRONMENT', 'development')}  \n",
    "\n",
    "## üìä Executive Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Tests Executed | {total_tests} |\n",
    "| Successful Tests | {total_successful} |\n",
    "| Success Rate | {(total_successful/total_tests*100):.1f}% |\n",
    "| Average Response Time | {perf_stats['avg_response_time']:.3f}s |\n",
    "| 95th Percentile Response Time | {perf_stats['p95_response_time']:.3f}s |\n",
    "| Tests Under 2s Response | {sum(1 for t in all_response_times if t < 2.0)/len(all_response_times)*100:.1f}% |\n",
    "\n",
    "## üéØ Test Categories\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add category details\n",
    "    for category, results in all_test_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        successful = sum(1 for r in results if r.get('success', False))\n",
    "        success_rate = (successful / len(results)) * 100\n",
    "        \n",
    "        # Calculate category response times\n",
    "        cat_response_times = [r['response_time'] for r in results \n",
    "                             if 'response_time' in r and r['response_time'] is not None]\n",
    "        avg_response = sum(cat_response_times) / len(cat_response_times) if cat_response_times else 0\n",
    "        \n",
    "        report += f\"\"\"\n",
    "### {category}\n",
    "\n",
    "- **Tests**: {len(results)}\n",
    "- **Success Rate**: {success_rate:.1f}%\n",
    "- **Average Response Time**: {avg_response:.3f}s\n",
    "- **Status**: {'‚úÖ Passing' if success_rate >= 80 else '‚ö†Ô∏è Needs Attention' if success_rate >= 50 else '‚ùå Failing'}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add failed tests\n",
    "        failed_tests = [r for r in results if not r.get('success', False)]\n",
    "        if failed_tests:\n",
    "            report += \"**Failed Tests:**\\n\"\n",
    "            for test in failed_tests[:5]:  # Show first 5 failures\n",
    "                test_name = test.get('test', 'Unknown test')\n",
    "                status_code = test.get('status_code', 'N/A')\n",
    "                error = test.get('error', 'No error details')\n",
    "                report += f\"- {test_name}: {status_code} - {error}\\n\"\n",
    "            \n",
    "            if len(failed_tests) > 5:\n",
    "                report += f\"- ... and {len(failed_tests) - 5} more failures\\n\"\n",
    "        \n",
    "        report += \"\\n\"\n",
    "    \n",
    "    # Add performance analysis\n",
    "    report += f\"\"\"\n",
    "## ‚ö° Performance Analysis\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Minimum Response Time | {perf_stats['min_response_time']:.3f}s |\n",
    "| Maximum Response Time | {perf_stats['max_response_time']:.3f}s |\n",
    "| Average Response Time | {perf_stats['avg_response_time']:.3f}s |\n",
    "| Median Response Time | {perf_stats['median_response_time']:.3f}s |\n",
    "| 95th Percentile | {perf_stats['p95_response_time']:.3f}s |\n",
    "| 99th Percentile | {perf_stats['p99_response_time']:.3f}s |\n",
    "\n",
    "### Performance Recommendations\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add performance recommendations\n",
    "    if perf_stats['avg_response_time'] > 2.0:\n",
    "        report += \"‚ùå **High Response Times**: Average response time exceeds 2s target\\n\"\n",
    "    elif perf_stats['avg_response_time'] > 1.0:\n",
    "        report += \"‚ö†Ô∏è **Moderate Response Times**: Consider optimization for better performance\\n\"\n",
    "    else:\n",
    "        report += \"‚úÖ **Good Performance**: Response times within acceptable range\\n\"\n",
    "    \n",
    "    if perf_stats['p95_response_time'] > 5.0:\n",
    "        report += \"‚ùå **Poor P95 Performance**: 95th percentile responses are too slow\\n\"\n",
    "    \n",
    "    # Add client metrics\n",
    "    report += f\"\"\"\n",
    "\n",
    "## üìà Client Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests Made | {client_metrics['requests_made']} |\n",
    "| Successful Requests | {client_metrics['requests_successful']} |\n",
    "| Failed Requests | {client_metrics['requests_failed']} |\n",
    "| Client Success Rate | {client_metrics.get('success_rate', 0):.1f}% |\n",
    "| Average Response Time | {client_metrics.get('average_response_time', 0):.3f}s |\n",
    "\n",
    "## üîç Detailed Test Results\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed test results table\n",
    "    report += \"| Test Category | Test Name | Status | Response Time | Status Code |\\n\"\n",
    "    report += \"|---------------|-----------|--------|---------------|-------------|\\n\"\n",
    "    \n",
    "    for category, results in all_test_results.items():\n",
    "        for result in results:\n",
    "            test_name = result.get('test', result.get('name', 'Unknown'))\n",
    "            status = '‚úÖ' if result.get('success', False) else '‚ùå'\n",
    "            response_time = f\"{result.get('response_time', 0):.3f}s\"\n",
    "            status_code = result.get('status_code', 'N/A')\n",
    "            \n",
    "            report += f\"| {category} | {test_name} | {status} | {response_time} | {status_code} |\\n\"\n",
    "    \n",
    "    # Add recommendations\n",
    "    report += f\"\"\"\n",
    "\n",
    "## üí° Recommendations\n",
    "\n",
    "### Immediate Actions\n",
    "\"\"\"\n",
    "    \n",
    "    if total_successful / total_tests < 0.8:\n",
    "        report += \"- üö® **Critical**: Success rate below 80% - investigate failing tests immediately\\n\"\n",
    "    \n",
    "    if perf_stats['avg_response_time'] > 2.0:\n",
    "        report += \"- ‚ö° **Performance**: Optimize response times - current average exceeds 2s target\\n\"\n",
    "    \n",
    "    # Check for specific issues\n",
    "    auth_tests = all_test_results.get('Authentication', [])\n",
    "    auth_success = sum(1 for r in auth_tests if r.get('success', False)) / len(auth_tests) if auth_tests else 1\n",
    "    \n",
    "    if auth_success < 0.8:\n",
    "        report += \"- üîê **Authentication**: Review authentication endpoints - low success rate\\n\"\n",
    "    \n",
    "    upload_tests = all_test_results.get('Resume Upload', [])\n",
    "    upload_success = sum(1 for r in upload_tests if r.get('success', False)) / len(upload_tests) if upload_tests else 1\n",
    "    \n",
    "    if upload_success < 0.8:\n",
    "        report += \"- üìÑ **File Upload**: File upload functionality needs attention\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "### Long-term Improvements\n",
    "- üìä **Monitoring**: Implement continuous performance monitoring\n",
    "- üîÑ **Automation**: Add these tests to CI/CD pipeline\n",
    "- üìà **Metrics**: Set up alerting for performance degradation\n",
    "- üß™ **Testing**: Expand test coverage for edge cases\n",
    "\n",
    "---\n",
    "\n",
    "**Report Generated by**: HR Resume Search MCP API Testing Suite  \n",
    "**Notebook**: `notebooks/api_testing.ipynb`  \n",
    "**Timestamp**: {timestamp}  \n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the report\n",
    "test_report = generate_detailed_report()\n",
    "\n",
    "# Save report to file\n",
    "report_file = f\"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(test_report)\n",
    "\n",
    "print(f\"‚úÖ Comprehensive test report saved to: {report_file}\")\n",
    "\n",
    "# Display report summary\n",
    "display(Markdown(\"## üìã Test Report Summary\\n\\n\" + test_report[:1000] + \"\\n\\n*Full report saved to file.*\"))\n",
    "\n",
    "# Clean up\n",
    "api_client.close()\n",
    "\n",
    "print(\"\\nüéâ API Testing Suite completed successfully!\")\n",
    "print(f\"üìä Total tests executed: {sum(len(results) for results in all_test_results.values())}\")\n",
    "print(f\"üìù Report saved: {report_file}\")\n",
    "print(f\"üßπ Resources cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary and Next Steps\n",
    "\n",
    "This comprehensive testing notebook has covered:\n",
    "\n",
    "### ‚úÖ Completed Tests\n",
    "1. **API Health Checks** - Verified basic connectivity and endpoint availability\n",
    "2. **Authentication Flow** - Tested login, token management, and protected endpoints\n",
    "3. **Resume Upload** - Tested file upload with various formats and Claude AI parsing\n",
    "4. **Search Functionality** - Comprehensive search testing with different query types\n",
    "5. **Performance Testing** - Load testing with concurrent requests and stress testing\n",
    "6. **Data Visualization** - Rich charts and analytics for all test results\n",
    "\n",
    "### üìä Key Features\n",
    "- **Interactive Testing** - Real-time API testing with immediate feedback\n",
    "- **Performance Metrics** - Detailed response time and success rate analysis\n",
    "- **Visual Analytics** - Comprehensive charts and dashboards\n",
    "- **Automated Reporting** - Generated markdown reports for documentation\n",
    "- **Error Handling** - Robust error detection and reporting\n",
    "\n",
    "### üöÄ Usage Instructions\n",
    "1. Ensure API server is running at `http://localhost:8000`\n",
    "2. Run notebook cells sequentially\n",
    "3. Monitor test results and visualizations\n",
    "4. Review generated test report\n",
    "5. Use insights for API improvement\n",
    "\n",
    "### üîß Customization\n",
    "- Modify `TEST_TIMEOUT` for different response time requirements\n",
    "- Adjust `CONCURRENT_REQUESTS` for different load testing scenarios\n",
    "- Update test data in helper functions for specific testing needs\n",
    "- Extend visualization functions for custom charts\n",
    "\n",
    "### üìà Next Steps\n",
    "- Integrate with CI/CD pipeline for automated testing\n",
    "- Add more sophisticated test scenarios\n",
    "- Implement test data factories for larger datasets\n",
    "- Create alerts based on performance thresholds\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Testing! üß™‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}