{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Resume Search MCP API - Comprehensive Testing Suite\n",
    "\n",
    "This notebook provides comprehensive testing and demonstration of the HR Resume Search MCP API.\n",
    "\n",
    "## Features Tested\n",
    "- ‚úÖ **Authentication Flow**: JWT login, token management, refresh\n",
    "- ‚úÖ **Resume Upload**: PDF/DOC/DOCX file processing with Claude AI\n",
    "- ‚úÖ **Search Functionality**: Smart candidate matching and filtering\n",
    "- ‚úÖ **Performance Testing**: Concurrent requests and load testing\n",
    "- ‚úÖ **Data Visualization**: Search results and performance metrics\n",
    "\n",
    "## Prerequisites\n",
    "1. API server running at `http://localhost:8000`\n",
    "2. Database and Redis services available\n",
    "3. Claude API key configured\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import time\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# IPython display\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed, IntSlider\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configure seaborn\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_HOST = os.getenv('API_HOST', 'localhost')\n",
    "API_PORT = os.getenv('API_PORT', '8000')\n",
    "API_PREFIX = os.getenv('API_PREFIX', '/api/v1')\n",
    "API_BASE_URL = f\"http://{API_HOST}:{API_PORT}\"\n",
    "API_URL = f\"{API_BASE_URL}{API_PREFIX}\"\n",
    "\n",
    "# Test configuration\n",
    "TEST_TIMEOUT = 30\n",
    "CONCURRENT_REQUESTS = 10\n",
    "LOAD_TEST_DURATION = 60  # seconds\n",
    "\n",
    "# Display configuration\n",
    "config_html = f\"\"\"\n",
    "<div style=\"background-color: #f0f8ff; padding: 15px; border-radius: 10px; border-left: 5px solid #007acc;\">\n",
    "    <h3 style=\"color: #007acc; margin-top: 0;\">üîß API Configuration</h3>\n",
    "    <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr><td><strong>Base URL:</strong></td><td>{API_BASE_URL}</td></tr>\n",
    "        <tr><td><strong>API URL:</strong></td><td>{API_URL}</td></tr>\n",
    "        <tr><td><strong>Timeout:</strong></td><td>{TEST_TIMEOUT}s</td></tr>\n",
    "        <tr><td><strong>Environment:</strong></td><td>{os.getenv('ENVIRONMENT', 'development')}</td></tr>\n",
    "    </table>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(config_html))\n",
    "print(f\"API Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Helper Functions and Test Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APITestClient:\n",
    "    \"\"\"Enhanced API testing client with authentication and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = API_URL):\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.Client(timeout=TEST_TIMEOUT)\n",
    "        self.async_client = None\n",
    "        self.auth_token = None\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        self.metrics = {\n",
    "            \"requests_made\": 0,\n",
    "            \"requests_successful\": 0,\n",
    "            \"requests_failed\": 0,\n",
    "            \"total_response_time\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def set_auth_token(self, token: str):\n",
    "        \"\"\"Set authentication token\"\"\"\n",
    "        self.auth_token = token\n",
    "        self.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    \n",
    "    def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Make HTTP request with metrics tracking\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\" if not endpoint.startswith('http') else endpoint\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = getattr(self.client, method.lower())(\n",
    "                url, headers=self.headers, **kwargs\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics[\"requests_made\"] += 1\n",
    "            self.metrics[\"total_response_time\"] += response_time\n",
    "            \n",
    "            if response.status_code < 400:\n",
    "                self.metrics[\"requests_successful\"] += 1\n",
    "            else:\n",
    "                self.metrics[\"requests_failed\"] += 1\n",
    "                self.metrics[\"errors\"].append({\n",
    "                    \"endpoint\": endpoint,\n",
    "                    \"status_code\": response.status_code,\n",
    "                    \"error\": response.text\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code < 400,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": response.json() if response.content else None,\n",
    "                \"headers\": dict(response.headers),\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            self.metrics[\"requests_made\"] += 1\n",
    "            self.metrics[\"requests_failed\"] += 1\n",
    "            self.metrics[\"total_response_time\"] += response_time\n",
    "            self.metrics[\"errors\"].append({\n",
    "                \"endpoint\": endpoint,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "    \n",
    "    def get(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"GET\", endpoint, **kwargs)\n",
    "    \n",
    "    def post(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"POST\", endpoint, **kwargs)\n",
    "    \n",
    "    def put(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"PUT\", endpoint, **kwargs)\n",
    "    \n",
    "    def delete(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        return self._make_request(\"DELETE\", endpoint, **kwargs)\n",
    "    \n",
    "    async def async_get(self, endpoint: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Async GET request\"\"\"\n",
    "        if not self.async_client:\n",
    "            self.async_client = httpx.AsyncClient(timeout=TEST_TIMEOUT)\n",
    "        \n",
    "        url = f\"{self.base_url}{endpoint}\" if not endpoint.startswith('http') else endpoint\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = await self.async_client.get(\n",
    "                url, headers=self.headers, **kwargs\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code < 400,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time\": response_time,\n",
    "                \"data\": response.json() if response.content else None,\n",
    "                \"url\": url\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance metrics\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        if metrics[\"requests_made\"] > 0:\n",
    "            metrics[\"average_response_time\"] = metrics[\"total_response_time\"] / metrics[\"requests_made\"]\n",
    "            metrics[\"success_rate\"] = (metrics[\"requests_successful\"] / metrics[\"requests_made\"]) * 100\n",
    "        else:\n",
    "            metrics[\"average_response_time\"] = 0\n",
    "            metrics[\"success_rate\"] = 0\n",
    "        return metrics\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset performance metrics\"\"\"\n",
    "        self.metrics = {\n",
    "            \"requests_made\": 0,\n",
    "            \"requests_successful\": 0,\n",
    "            \"requests_failed\": 0,\n",
    "            \"total_response_time\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close client connections\"\"\"\n",
    "        self.client.close()\n",
    "        if self.async_client:\n",
    "            asyncio.run(self.async_client.aclose())\n",
    "\n",
    "# Initialize test client\n",
    "api_client = APITestClient()\n",
    "\n",
    "print(\"‚úÖ APITestClient initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_resume_files():\n",
    "    \"\"\"Create sample resume files for testing\"\"\"\n",
    "    sample_resumes = {\n",
    "        \"john_doe_resume.json\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"email\": \"john.doe@example.com\",\n",
    "            \"phone\": \"+1-555-0123\",\n",
    "            \"location\": \"New York, NY\",\n",
    "            \"summary\": \"Experienced software engineer with 5+ years in full-stack development\",\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"company\": \"Tech Corp\",\n",
    "                    \"position\": \"Senior Software Engineer\",\n",
    "                    \"department\": \"Engineering\",\n",
    "                    \"desk\": \"Platform Team\",\n",
    "                    \"start_date\": \"2020-01-01\",\n",
    "                    \"end_date\": None,\n",
    "                    \"description\": \"Leading platform development initiatives\"\n",
    "                },\n",
    "                {\n",
    "                    \"company\": \"StartupXYZ\",\n",
    "                    \"position\": \"Software Engineer\",\n",
    "                    \"department\": \"Product\",\n",
    "                    \"desk\": \"Backend Team\",\n",
    "                    \"start_date\": \"2018-06-01\",\n",
    "                    \"end_date\": \"2019-12-31\",\n",
    "                    \"description\": \"Developed scalable backend services\"\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"Docker\", \"AWS\", \"PostgreSQL\"],\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"institution\": \"University of Technology\",\n",
    "                    \"degree\": \"Bachelor of Science\",\n",
    "                    \"field_of_study\": \"Computer Science\",\n",
    "                    \"graduation_date\": \"2018-05-01\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"jane_smith_resume.json\": {\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"email\": \"jane.smith@example.com\",\n",
    "            \"phone\": \"+1-555-0456\",\n",
    "            \"location\": \"San Francisco, CA\",\n",
    "            \"summary\": \"Product manager with expertise in data analytics and machine learning\",\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"company\": \"Tech Corp\",\n",
    "                    \"position\": \"Senior Product Manager\",\n",
    "                    \"department\": \"Product\",\n",
    "                    \"desk\": \"AI Team\",\n",
    "                    \"start_date\": \"2019-03-01\",\n",
    "                    \"end_date\": None,\n",
    "                    \"description\": \"Leading AI product initiatives\"\n",
    "                },\n",
    "                {\n",
    "                    \"company\": \"DataCorp\",\n",
    "                    \"position\": \"Data Analyst\",\n",
    "                    \"department\": \"Analytics\",\n",
    "                    \"desk\": \"Business Intelligence\",\n",
    "                    \"start_date\": \"2017-01-01\",\n",
    "                    \"end_date\": \"2019-02-28\",\n",
    "                    \"description\": \"Analyzed business metrics and created dashboards\"\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\"Python\", \"SQL\", \"Tableau\", \"Machine Learning\", \"Product Strategy\"],\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"institution\": \"Stanford University\",\n",
    "                    \"degree\": \"Master of Science\",\n",
    "                    \"field_of_study\": \"Data Science\",\n",
    "                    \"graduation_date\": \"2016-06-01\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create sample files directory\n",
    "    sample_dir = Path(\"sample_resumes\")\n",
    "    sample_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save sample files\n",
    "    for filename, data in sample_resumes.items():\n",
    "        with open(sample_dir / filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    return sample_dir, list(sample_resumes.keys())\n",
    "\n",
    "def visualize_test_results(results: List[Dict], title: str = \"Test Results\"):\n",
    "    \"\"\"Visualize test results with multiple charts\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Response Times', 'Success Rate', 'Status Codes', 'Timeline'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Response times histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df['response_time'], name=\"Response Time\", nbinsx=20),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Success rate pie chart\n",
    "    success_counts = df['success'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=['Success', 'Failed'], values=[success_counts.get(True, 0), success_counts.get(False, 0)]),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Status codes bar chart\n",
    "    status_counts = df['status_code'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=status_counts.index.astype(str), y=status_counts.values, name=\"Status Codes\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Timeline scatter plot\n",
    "    if 'timestamp' in df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df.index, y=df['response_time'], mode='lines+markers', name=\"Response Time Timeline\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=title, showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    stats_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "        <h4>üìä Test Summary Statistics</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px;\">\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #007acc;\">{len(df)}</div>\n",
    "                <div>Total Requests</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #28a745;\">{df['success'].sum()}</div>\n",
    "                <div>Successful</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #dc3545;\">{(~df['success']).sum()}</div>\n",
    "                <div>Failed</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 10px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #6f42c1;\">{df['response_time'].mean():.3f}s</div>\n",
    "                <div>Avg Response Time</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(stats_html))\n",
    "\n",
    "# Create sample data\n",
    "sample_dir, sample_files = create_sample_resume_files()\n",
    "print(f\"‚úÖ Created sample resume files: {sample_files}\")\n",
    "print(f\"‚úÖ Helper functions initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç API Health Check\n",
    "\n",
    "First, let's verify that the API is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic API connectivity\n",
    "print(\"üîç Testing API connectivity...\")\n",
    "\n",
    "health_endpoints = [\n",
    "    (\"/\", \"Root endpoint\"),\n",
    "    (\"/health\", \"Health check\"),\n",
    "    (\"/readiness\", \"Readiness check\"),\n",
    "    (\"/docs\", \"API documentation\"),\n",
    "    (\"/openapi.json\", \"OpenAPI schema\")\n",
    "]\n",
    "\n",
    "health_results = []\n",
    "\n",
    "for endpoint, description in health_endpoints:\n",
    "    # Use base URL for root-level endpoints\n",
    "    test_url = f\"{API_BASE_URL}{endpoint}\" if endpoint in [\"/\", \"/health\", \"/readiness\", \"/docs\", \"/openapi.json\"] else endpoint\n",
    "    result = api_client.get(test_url)\n",
    "    \n",
    "    health_results.append({\n",
    "        \"endpoint\": endpoint,\n",
    "        \"description\": description,\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"success\": result[\"success\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} {endpoint} ({description}): {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "# Visualize health check results\n",
    "visualize_test_results(health_results, \"API Health Check Results\")\n",
    "\n",
    "# Check if API is ready\n",
    "api_ready = all(result[\"success\"] for result in health_results if result[\"endpoint\"] in [\"/\", \"/health\"])\n",
    "if api_ready:\n",
    "    print(\"\\nüéâ API is ready for testing!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è API may not be fully ready. Some tests may fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîê Authentication Flow Testing\n",
    "\n",
    "Testing JWT authentication, login, token refresh, and logout flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication test data\n",
    "test_users = {\n",
    "    \"admin\": {\n",
    "        \"email\": \"admin@example.com\",\n",
    "        \"password\": \"admin123\",\n",
    "        \"role\": \"admin\"\n",
    "    },\n",
    "    \"hr_manager\": {\n",
    "        \"email\": \"hr@example.com\",\n",
    "        \"password\": \"hr123\",\n",
    "        \"role\": \"hr_manager\"\n",
    "    },\n",
    "    \"recruiter\": {\n",
    "        \"email\": \"recruiter@example.com\",\n",
    "        \"password\": \"recruiter123\",\n",
    "        \"role\": \"recruiter\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîê Testing Authentication Flow...\")\n",
    "auth_results = []\n",
    "\n",
    "# Test user registration (if endpoint exists)\n",
    "print(\"\\n1. Testing User Registration\")\n",
    "for username, user_data in test_users.items():\n",
    "    register_data = {\n",
    "        \"email\": user_data[\"email\"],\n",
    "        \"password\": user_data[\"password\"],\n",
    "        \"role\": user_data[\"role\"]\n",
    "    }\n",
    "    \n",
    "    result = api_client.post(\"/auth/register\", json=register_data)\n",
    "    auth_results.append({\n",
    "        \"test\": f\"Register {username}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] or result[\"status_code\"] == 409 else \"‚ùå\"  # 409 = user exists\n",
    "    print(f\"{status_icon} Register {username}: {result['status_code']}\")\n",
    "\n",
    "# Test login\n",
    "print(\"\\n2. Testing User Login\")\n",
    "successful_logins = {}\n",
    "\n",
    "for username, user_data in test_users.items():\n",
    "    login_data = {\n",
    "        \"email\": user_data[\"email\"],\n",
    "        \"password\": user_data[\"password\"]\n",
    "    }\n",
    "    \n",
    "    result = api_client.post(\"/auth/login\", json=login_data)\n",
    "    auth_results.append({\n",
    "        \"test\": f\"Login {username}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        token = result[\"data\"].get(\"access_token\")\n",
    "        if token:\n",
    "            successful_logins[username] = {\n",
    "                \"token\": token,\n",
    "                \"user_data\": result[\"data\"]\n",
    "            }\n",
    "            print(f\"‚úÖ Login {username}: Success - Token obtained\")\n",
    "        else:\n",
    "            print(f\"‚ùå Login {username}: No token in response\")\n",
    "    else:\n",
    "        print(f\"‚ùå Login {username}: {result.get('status_code', 'Unknown error')}\")\n",
    "\n",
    "# Test authenticated endpoints\n",
    "print(\"\\n3. Testing Authenticated Endpoints\")\n",
    "if successful_logins:\n",
    "    # Use first successful login for testing\n",
    "    test_user = list(successful_logins.keys())[0]\n",
    "    test_token = successful_logins[test_user][\"token\"]\n",
    "    \n",
    "    # Set token for API client\n",
    "    api_client.set_auth_token(test_token)\n",
    "    \n",
    "    # Test protected endpoints\n",
    "    protected_endpoints = [\n",
    "        \"/auth/me\",\n",
    "        \"/resumes\",\n",
    "        \"/search\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in protected_endpoints:\n",
    "        result = api_client.get(endpoint)\n",
    "        auth_results.append({\n",
    "            \"test\": f\"Protected {endpoint}\",\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} {endpoint}: {result['status_code']}\")\n",
    "    \n",
    "    print(f\"\\nüîë Using token for {test_user}: {test_token[:20]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No successful logins - cannot test protected endpoints\")\n",
    "\n",
    "# Display authentication test results\n",
    "print(\"\\nüìä Authentication Test Summary:\")\n",
    "visualize_test_results(auth_results, \"Authentication Flow Test Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Resume Upload Testing\n",
    "\n",
    "Testing file upload functionality with various formats and Claude AI parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÑ Testing Resume Upload Functionality...\")\n",
    "upload_results = []\n",
    "\n",
    "# Create test files in different formats\n",
    "def create_test_resume_content():\n",
    "    return \"\"\"\n",
    "JOHN DOE\n",
    "Software Engineer\n",
    "Email: john.doe@example.com\n",
    "Phone: +1-555-0123\n",
    "Location: New York, NY\n",
    "\n",
    "SUMMARY\n",
    "Experienced software engineer with 5+ years in full-stack development.\n",
    "Expertise in Python, JavaScript, and cloud technologies.\n",
    "\n",
    "EXPERIENCE\n",
    "Senior Software Engineer | Tech Corp | 2020-Present\n",
    "- Lead platform development initiatives\n",
    "- Manage team of 5 developers\n",
    "- Implemented microservices architecture\n",
    "\n",
    "Software Engineer | StartupXYZ | 2018-2019\n",
    "- Developed scalable backend services\n",
    "- Built REST APIs and databases\n",
    "- Worked with Docker and Kubernetes\n",
    "\n",
    "SKILLS\n",
    "Programming: Python, JavaScript, Go, SQL\n",
    "Technologies: Docker, Kubernetes, AWS, PostgreSQL\n",
    "Frameworks: FastAPI, React, Node.js\n",
    "\n",
    "EDUCATION\n",
    "Bachelor of Science in Computer Science\n",
    "University of Technology | 2018\n",
    "\"\"\"\n",
    "\n",
    "# Create test files\n",
    "test_files_dir = Path(\"test_uploads\")\n",
    "test_files_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create text file (simulating converted PDF content)\n",
    "test_content = create_test_resume_content()\n",
    "test_files = []\n",
    "\n",
    "# Text file\n",
    "text_file = test_files_dir / \"john_doe_resume.txt\"\n",
    "with open(text_file, 'w') as f:\n",
    "    f.write(test_content)\n",
    "test_files.append((\"john_doe_resume.txt\", \"text/plain\"))\n",
    "\n",
    "# JSON file (structured resume)\n",
    "json_file = test_files_dir / \"jane_smith_resume.json\"\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Jane Smith\",\n",
    "        \"email\": \"jane.smith@example.com\",\n",
    "        \"position\": \"Product Manager\",\n",
    "        \"experience\": [\n",
    "            {\n",
    "                \"company\": \"Tech Corp\",\n",
    "                \"role\": \"Senior Product Manager\",\n",
    "                \"department\": \"Product\",\n",
    "                \"years\": 3\n",
    "            }\n",
    "        ],\n",
    "        \"skills\": [\"Product Strategy\", \"Data Analysis\", \"Python\", \"SQL\"]\n",
    "    }, f, indent=2)\n",
    "test_files.append((\"jane_smith_resume.json\", \"application/json\"))\n",
    "\n",
    "print(f\"Created test files: {[f[0] for f in test_files]}\")\n",
    "\n",
    "# Test file uploads\n",
    "print(\"\\n1. Testing File Upload Endpoints\")\n",
    "uploaded_resumes = []\n",
    "\n",
    "for filename, content_type in test_files:\n",
    "    file_path = test_files_dir / filename\n",
    "    \n",
    "    # Test upload\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            files = {'file': (filename, f, content_type)}\n",
    "            # Note: files parameter bypasses the json content-type header\n",
    "            result = api_client.client.post(\n",
    "                f\"{api_client.base_url}/resumes/upload\",\n",
    "                files=files,\n",
    "                headers={\"Authorization\": api_client.headers.get(\"Authorization\", \"\")}\n",
    "            )\n",
    "            \n",
    "            upload_result = {\n",
    "                \"success\": result.status_code < 400,\n",
    "                \"status_code\": result.status_code,\n",
    "                \"response_time\": 0,  # Will be updated by client metrics\n",
    "                \"data\": result.json() if result.content else None,\n",
    "                \"filename\": filename\n",
    "            }\n",
    "            \n",
    "            upload_results.append({\n",
    "                \"test\": f\"Upload {filename}\",\n",
    "                \"success\": upload_result[\"success\"],\n",
    "                \"status_code\": upload_result[\"status_code\"],\n",
    "                \"response_time\": upload_result[\"response_time\"],\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"file_type\": content_type\n",
    "            })\n",
    "            \n",
    "            if upload_result[\"success\"] and upload_result[\"data\"]:\n",
    "                resume_id = upload_result[\"data\"].get(\"id\")\n",
    "                if resume_id:\n",
    "                    uploaded_resumes.append({\n",
    "                        \"id\": resume_id,\n",
    "                        \"filename\": filename,\n",
    "                        \"data\": upload_result[\"data\"]\n",
    "                    })\n",
    "                    print(f\"‚úÖ Uploaded {filename}: ID {resume_id}\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ Uploaded {filename}: No ID returned\")\n",
    "            else:\n",
    "                print(f\"‚ùå Upload {filename}: {upload_result['status_code']}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload {filename}: Error - {str(e)}\")\n",
    "        upload_results.append({\n",
    "            \"test\": f\"Upload {filename}\",\n",
    "            \"success\": False,\n",
    "            \"status_code\": None,\n",
    "            \"response_time\": 0,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Test resume parsing status\n",
    "print(\"\\n2. Testing Resume Parsing Status\")\n",
    "for resume in uploaded_resumes:\n",
    "    result = api_client.get(f\"/resumes/{resume['id']}\")\n",
    "    upload_results.append({\n",
    "        \"test\": f\"Get resume {resume['id']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        parse_status = result[\"data\"].get(\"parse_status\", \"unknown\")\n",
    "        print(f\"‚úÖ Resume {resume['id']}: Parse status = {parse_status}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Resume {resume['id']}: Could not get status\")\n",
    "\n",
    "# Test Claude AI parsing (if available)\n",
    "print(\"\\n3. Testing Claude AI Parsing\")\n",
    "for resume in uploaded_resumes:\n",
    "    result = api_client.get(f\"/resumes/{resume['id']}/parse\")\n",
    "    upload_results.append({\n",
    "        \"test\": f\"Parse resume {resume['id']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} Parse resume {resume['id']}: {result['status_code']}\")\n",
    "    \n",
    "    if result[\"success\"] and result[\"data\"]:\n",
    "        parsed_data = result[\"data\"]\n",
    "        print(f\"   Parsed fields: {list(parsed_data.keys())}\")\n",
    "\n",
    "# Display upload test results\n",
    "print(\"\\nüìä Resume Upload Test Summary:\")\n",
    "visualize_test_results(upload_results, \"Resume Upload Test Results\")\n",
    "\n",
    "# Store uploaded resume IDs for search testing\n",
    "resume_ids = [r[\"id\"] for r in uploaded_resumes]\n",
    "print(f\"\\nüìù Uploaded resume IDs for search testing: {resume_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Search Functionality Testing\n",
    "\n",
    "Testing various search capabilities including smart matching and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Testing Search Functionality...\")\n",
    "search_results = []\n",
    "search_data = []\n",
    "\n",
    "# Define test search queries\n",
    "search_queries = [\n",
    "    {\n",
    "        \"name\": \"Basic keyword search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"q\": \"software engineer\"},\n",
    "        \"description\": \"Search for software engineers\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Skills-based search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"skills\": \"Python,JavaScript\"},\n",
    "        \"description\": \"Search by specific skills\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Department search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"department\": \"Engineering\"},\n",
    "        \"description\": \"Search by department\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Experience range\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"min_experience\": 3, \"max_experience\": 8},\n",
    "        \"description\": \"Search by experience range\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Location search\",\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"params\": {\"location\": \"New York\"},\n",
    "        \"description\": \"Search by location\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Advanced search\",\n",
    "        \"endpoint\": \"/search/advanced\",\n",
    "        \"method\": \"POST\",\n",
    "        \"data\": {\n",
    "            \"query\": \"senior engineer\",\n",
    "            \"filters\": {\n",
    "                \"skills\": [\"Python\", \"AWS\"],\n",
    "                \"experience_years\": {\"min\": 3, \"max\": 10},\n",
    "                \"departments\": [\"Engineering\", \"Product\"]\n",
    "            },\n",
    "            \"sort_by\": \"relevance\",\n",
    "            \"limit\": 20\n",
    "        },\n",
    "        \"description\": \"Advanced search with multiple filters\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test similar candidates search (if we have uploaded resumes)\n",
    "if resume_ids:\n",
    "    for resume_id in resume_ids[:2]:  # Test first 2 resumes\n",
    "        search_queries.append({\n",
    "            \"name\": f\"Similar candidates to {resume_id}\",\n",
    "            \"endpoint\": f\"/search/similar/{resume_id}\",\n",
    "            \"params\": {},\n",
    "            \"description\": f\"Find candidates similar to resume {resume_id}\"\n",
    "        })\n",
    "\n",
    "# Execute search tests\n",
    "print(\"\\n1. Testing Search Endpoints\")\n",
    "for query in search_queries:\n",
    "    try:\n",
    "        if query.get(\"method\") == \"POST\":\n",
    "            result = api_client.post(query[\"endpoint\"], json=query.get(\"data\", {}))\n",
    "        else:\n",
    "            result = api_client.get(query[\"endpoint\"], params=query.get(\"params\", {}))\n",
    "        \n",
    "        search_results.append({\n",
    "            \"test\": query[\"name\"],\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"endpoint\": query[\"endpoint\"]\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} {query['name']}: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "        \n",
    "        # Store search data for visualization\n",
    "        if result[\"success\"] and result[\"data\"]:\n",
    "            results_data = result[\"data\"].get(\"results\", [])\n",
    "            total_count = result[\"data\"].get(\"total\", 0)\n",
    "            \n",
    "            search_data.append({\n",
    "                \"query_name\": query[\"name\"],\n",
    "                \"total_results\": total_count,\n",
    "                \"returned_results\": len(results_data),\n",
    "                \"response_time\": result[\"response_time\"],\n",
    "                \"results\": results_data[:5]  # Store first 5 results for analysis\n",
    "            })\n",
    "            \n",
    "            print(f\"   Found {total_count} total results, returned {len(results_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {query['name']}: Error - {str(e)}\")\n",
    "        search_results.append({\n",
    "            \"test\": query[\"name\"],\n",
    "            \"success\": False,\n",
    "            \"status_code\": None,\n",
    "            \"response_time\": 0,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Test professional network search\n",
    "print(\"\\n2. Testing Professional Network Search\")\n",
    "if resume_ids:\n",
    "    for resume_id in resume_ids[:1]:  # Test first resume\n",
    "        result = api_client.get(f\"/search/network/{resume_id}\")\n",
    "        search_results.append({\n",
    "            \"test\": f\"Network search for {resume_id}\",\n",
    "            \"success\": result[\"success\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status_icon} Network search for {resume_id}: {result['status_code']}\")\n",
    "\n",
    "# Test search filters and pagination\n",
    "print(\"\\n3. Testing Search Filters and Pagination\")\n",
    "pagination_tests = [\n",
    "    {\"limit\": 5, \"offset\": 0},\n",
    "    {\"limit\": 10, \"offset\": 5},\n",
    "    {\"limit\": 20, \"offset\": 0}\n",
    "]\n",
    "\n",
    "for params in pagination_tests:\n",
    "    result = api_client.get(\"/search\", params={\"q\": \"engineer\", **params})\n",
    "    search_results.append({\n",
    "        \"test\": f\"Pagination limit={params['limit']} offset={params['offset']}\",\n",
    "        \"success\": result[\"success\"],\n",
    "        \"status_code\": result[\"status_code\"],\n",
    "        \"response_time\": result[\"response_time\"],\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"{status_icon} Pagination test: {result['status_code']}\")\n",
    "\n",
    "# Visualize search test results\n",
    "print(\"\\nüìä Search Test Summary:\")\n",
    "visualize_test_results(search_results, \"Search Functionality Test Results\")\n",
    "\n",
    "# Create search results visualization\n",
    "if search_data:\n",
    "    search_df = pd.DataFrame(search_data)\n",
    "    \n",
    "    # Search results chart\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Results count by query\n",
    "    ax1.bar(range(len(search_df)), search_df['total_results'])\n",
    "    ax1.set_title('Search Results Count by Query')\n",
    "    ax1.set_xlabel('Query Index')\n",
    "    ax1.set_ylabel('Total Results')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Response time by query\n",
    "    ax2.plot(search_df['response_time'], marker='o', linewidth=2, markersize=6)\n",
    "    ax2.set_title('Search Response Times')\n",
    "    ax2.set_xlabel('Query Index')\n",
    "    ax2.set_ylabel('Response Time (seconds)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Results distribution\n",
    "    ax3.hist(search_df['total_results'], bins=10, edgecolor='black', alpha=0.7)\n",
    "    ax3.set_title('Distribution of Search Result Counts')\n",
    "    ax3.set_xlabel('Number of Results')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # Performance vs results\n",
    "    ax4.scatter(search_df['total_results'], search_df['response_time'], \n",
    "                s=100, alpha=0.6, c=range(len(search_df)), cmap='viridis')\n",
    "    ax4.set_title('Response Time vs Results Count')\n",
    "    ax4.set_xlabel('Total Results')\n",
    "    ax4.set_ylabel('Response Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Analyzed {len(search_data)} search queries with results\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No search data available for visualization\")\n",
    "\n",
    "print(f\"\\nüîç Search testing completed - {len(search_results)} tests executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üîç Enhanced Search Functionality Testing\n\nTesting sophisticated search capabilities including multi-criteria search, similar profile matching, colleague discovery, and AI-powered search with comprehensive test datasets.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° Performance Testing\n",
    "\n",
    "Testing API performance with concurrent requests and load testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Testing API Performance...\")\n",
    "\n",
    "async def make_concurrent_requests(endpoint: str, num_requests: int = 10, params: dict = None):\n",
    "    \"\"\"Make concurrent requests to test load handling\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    async def make_request(request_id: int):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = await api_client.async_get(endpoint, params=params or {})\n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"success\": result[\"success\"],\n",
    "                \"status_code\": result[\"status_code\"],\n",
    "                \"response_time\": result[\"response_time\"],\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"endpoint\": endpoint\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"endpoint\": endpoint,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    # Execute concurrent requests\n",
    "    tasks = [make_request(i) for i in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Performance test endpoints\n",
    "perf_endpoints = [\n",
    "    {\n",
    "        \"endpoint\": \"/health\",\n",
    "        \"name\": \"Health check\",\n",
    "        \"params\": None,\n",
    "        \"concurrent_requests\": 20\n",
    "    },\n",
    "    {\n",
    "        \"endpoint\": \"/search\",\n",
    "        \"name\": \"Basic search\",\n",
    "        \"params\": {\"q\": \"engineer\"},\n",
    "        \"concurrent_requests\": 15\n",
    "    },\n",
    "    {\n",
    "        \"endpoint\": \"/resumes\",\n",
    "        \"name\": \"Resume list\",\n",
    "        \"params\": {\"limit\": 10},\n",
    "        \"concurrent_requests\": 10\n",
    "    }\n",
    "]\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "print(\"\\n1. Testing Concurrent Request Handling\")\n",
    "for test_config in perf_endpoints:\n",
    "    print(f\"\\nTesting {test_config['name']} with {test_config['concurrent_requests']} concurrent requests...\")\n",
    "    \n",
    "    # Adjust endpoint for base URL if needed\n",
    "    endpoint = test_config['endpoint']\n",
    "    if endpoint in [\"/health\", \"/readiness\"]:\n",
    "        endpoint = f\"{API_BASE_URL}{endpoint}\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = await make_concurrent_requests(\n",
    "            endpoint,\n",
    "            test_config['concurrent_requests'],\n",
    "            test_config['params']\n",
    "        )\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Analyze results\n",
    "        successful_requests = sum(1 for r in results if r['success'])\n",
    "        failed_requests = len(results) - successful_requests\n",
    "        avg_response_time = sum(r['response_time'] for r in results) / len(results)\n",
    "        max_response_time = max(r['response_time'] for r in results)\n",
    "        min_response_time = min(r['response_time'] for r in results)\n",
    "        \n",
    "        # Store performance data\n",
    "        perf_data = {\n",
    "            \"test\": f\"Concurrent {test_config['name']}\",\n",
    "            \"endpoint\": test_config['endpoint'],\n",
    "            \"concurrent_requests\": test_config['concurrent_requests'],\n",
    "            \"successful_requests\": successful_requests,\n",
    "            \"failed_requests\": failed_requests,\n",
    "            \"success_rate\": (successful_requests / len(results)) * 100,\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_response_time\": avg_response_time,\n",
    "            \"max_response_time\": max_response_time,\n",
    "            \"min_response_time\": min_response_time,\n",
    "            \"requests_per_second\": len(results) / total_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        performance_results.append(perf_data)\n",
    "        \n",
    "        # Add individual results\n",
    "        for result in results:\n",
    "            result['test_name'] = test_config['name']\n",
    "        \n",
    "        print(f\"‚úÖ {test_config['name']}:\")\n",
    "        print(f\"   Success rate: {perf_data['success_rate']:.1f}%\")\n",
    "        print(f\"   Avg response time: {avg_response_time:.3f}s\")\n",
    "        print(f\"   Requests/second: {perf_data['requests_per_second']:.1f}\")\n",
    "        print(f\"   Total time: {total_time:.3f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {test_config['name']}: Error - {str(e)}\")\n",
    "        performance_results.append({\n",
    "            \"test\": f\"Concurrent {test_config['name']}\",\n",
    "            \"endpoint\": test_config['endpoint'],\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "# Stress testing with increasing load\n",
    "print(\"\\n2. Stress Testing with Increasing Load\")\n",
    "stress_loads = [5, 10, 20, 30, 50]\n",
    "stress_results = []\n",
    "\n",
    "for load in stress_loads:\n",
    "    print(f\"Testing with {load} concurrent requests...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = await make_concurrent_requests(f\"{API_BASE_URL}/health\", load)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        avg_time = sum(r['response_time'] for r in results) / len(results)\n",
    "        \n",
    "        stress_data = {\n",
    "            \"load\": load,\n",
    "            \"success_rate\": (successful / len(results)) * 100,\n",
    "            \"avg_response_time\": avg_time,\n",
    "            \"requests_per_second\": len(results) / total_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        stress_results.append(stress_data)\n",
    "        \n",
    "        print(f\"   Success: {stress_data['success_rate']:.1f}%, \"\n",
    "              f\"Avg time: {avg_time:.3f}s, \"\n",
    "              f\"RPS: {stress_data['requests_per_second']:.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load {load}: Error - {str(e)}\")\n",
    "\n",
    "# Visualize performance results\n",
    "print(\"\\nüìä Performance Test Visualization\")\n",
    "\n",
    "if performance_results and stress_results:\n",
    "    # Create performance dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Response Time by Endpoint',\n",
    "            'Success Rate vs Load',\n",
    "            'Requests per Second',\n",
    "            'Load Testing Results'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Response time by endpoint\n",
    "    endpoints = [p['endpoint'] for p in performance_results if 'avg_response_time' in p]\n",
    "    response_times = [p['avg_response_time'] for p in performance_results if 'avg_response_time' in p]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=endpoints, y=response_times, name=\"Response Time\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Success rate vs load (stress test)\n",
    "    loads = [s['load'] for s in stress_results]\n",
    "    success_rates = [s['success_rate'] for s in stress_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=loads, y=success_rates, mode='lines+markers', name=\"Success Rate\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Requests per second\n",
    "    rps_values = [p['requests_per_second'] for p in performance_results if 'requests_per_second' in p]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=endpoints, y=rps_values, name=\"RPS\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Load testing timeline\n",
    "    avg_times = [s['avg_response_time'] for s in stress_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=loads, y=avg_times, mode='lines+markers', name=\"Avg Response Time\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"API Performance Test Results\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    if performance_results:\n",
    "        best_perf = min(performance_results, key=lambda x: x.get('avg_response_time', float('inf')))\n",
    "        worst_perf = max(performance_results, key=lambda x: x.get('avg_response_time', 0))\n",
    "        \n",
    "        perf_summary = f\"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h4>‚ö° Performance Summary</h4>\n",
    "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;\">\n",
    "                <div>\n",
    "                    <h5 style=\"color: #28a745;\">üèÜ Best Performance</h5>\n",
    "                    <p><strong>Endpoint:</strong> {best_perf.get('endpoint', 'N/A')}</p>\n",
    "                    <p><strong>Avg Response:</strong> {best_perf.get('avg_response_time', 0):.3f}s</p>\n",
    "                    <p><strong>Success Rate:</strong> {best_perf.get('success_rate', 0):.1f}%</p>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h5 style=\"color: #dc3545;\">üêå Slowest Performance</h5>\n",
    "                    <p><strong>Endpoint:</strong> {worst_perf.get('endpoint', 'N/A')}</p>\n",
    "                    <p><strong>Avg Response:</strong> {worst_perf.get('avg_response_time', 0):.3f}s</p>\n",
    "                    <p><strong>Success Rate:</strong> {worst_perf.get('success_rate', 0):.1f}%</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(perf_summary))\n",
    "\n",
    "print(f\"\\n‚ö° Performance testing completed - {len(performance_results)} endpoint tests, {len(stress_results)} stress tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Data Visualization and Analytics\n",
    "\n",
    "Comprehensive visualization of test results and API analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating Comprehensive Data Visualizations...\")\n",
    "\n",
    "# Compile all test results\n",
    "all_test_results = {\n",
    "    \"Health Checks\": health_results,\n",
    "    \"Authentication\": auth_results,\n",
    "    \"Resume Upload\": upload_results,\n",
    "    \"Search Functionality\": search_results,\n",
    "    \"Performance Tests\": performance_results\n",
    "}\n",
    "\n",
    "# Create comprehensive analytics dashboard\n",
    "def create_analytics_dashboard():\n",
    "    \"\"\"Create a comprehensive analytics dashboard\"\"\"\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_results = []\n",
    "    for category, results in all_test_results.items():\n",
    "        for result in results:\n",
    "            result_copy = result.copy()\n",
    "            result_copy['category'] = category\n",
    "            combined_results.append(result_copy)\n",
    "    \n",
    "    if not combined_results:\n",
    "        print(\"‚ö†Ô∏è No test results available for visualization\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(combined_results)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Test Results by Category',\n",
    "            'Success Rate by Category',\n",
    "            'Response Time Distribution',\n",
    "            'Timeline Analysis',\n",
    "            'Status Code Distribution',\n",
    "            'Performance Over Time',\n",
    "            'Test Volume by Category',\n",
    "            'Error Analysis',\n",
    "            'API Health Score'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}, {\"type\": \"histogram\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"indicator\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Test results by category\n",
    "    category_success = df.groupby('category')['success'].agg(['sum', 'count']).reset_index()\n",
    "    category_success['success_rate'] = (category_success['sum'] / category_success['count']) * 100\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=category_success['category'],\n",
    "            y=category_success['success_rate'],\n",
    "            name=\"Success Rate\",\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Overall success rate pie chart\n",
    "    total_success = df['success'].sum()\n",
    "    total_tests = len(df)\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=['Success', 'Failed'],\n",
    "            values=[total_success, total_tests - total_success],\n",
    "            marker_colors=['lightgreen', 'lightcoral']\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Response time distribution\n",
    "    response_times = df['response_time'].dropna()\n",
    "    if len(response_times) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=response_times, nbinsx=20, name=\"Response Times\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # 4. Timeline analysis\n",
    "    if 'timestamp' in df.columns:\n",
    "        df_sorted = df.sort_values('timestamp')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_sorted.index,\n",
    "                y=df_sorted['response_time'],\n",
    "                mode='lines+markers',\n",
    "                name=\"Response Time Timeline\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 5. Status code distribution\n",
    "    status_codes = df['status_code'].dropna().astype(str)\n",
    "    if len(status_codes) > 0:\n",
    "        status_counts = status_codes.value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=status_counts.index, y=status_counts.values, name=\"Status Codes\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 6. Performance by category\n",
    "    if 'response_time' in df.columns:\n",
    "        category_perf = df.groupby('category')['response_time'].mean().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=category_perf['category'],\n",
    "                y=category_perf['response_time'],\n",
    "                mode='markers',\n",
    "                marker_size=15,\n",
    "                name=\"Avg Response Time\"\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # 7. Test volume by category\n",
    "    category_counts = df['category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=category_counts.index, y=category_counts.values, name=\"Test Count\"),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Error analysis\n",
    "    error_df = df[df['success'] == False]\n",
    "    if len(error_df) > 0:\n",
    "        error_categories = error_df['category'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=error_categories.index, y=error_categories.values, \n",
    "                   name=\"Errors\", marker_color='red'),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 9. API Health Score\n",
    "    health_score = (total_success / total_tests) * 100 if total_tests > 0 else 0\n",
    "    avg_response_time = df['response_time'].mean() if 'response_time' in df.columns else 0\n",
    "    \n",
    "    # Calculate composite health score\n",
    "    response_score = max(0, 100 - (avg_response_time * 50))  # Penalty for slow responses\n",
    "    composite_score = (health_score * 0.7) + (response_score * 0.3)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=composite_score,\n",
    "            domain={'x': [0, 1], 'y': [0, 1]},\n",
    "            title={'text': \"API Health Score\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [None, 100]},\n",
    "                'bar': {'color': \"darkblue\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                    {'range': [50, 80], 'color': \"yellow\"},\n",
    "                    {'range': [80, 100], 'color': \"green\"}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 90\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        row=3, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"HR Resume Search MCP API - Comprehensive Test Analytics Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return df, {\n",
    "        'total_tests': total_tests,\n",
    "        'successful_tests': total_success,\n",
    "        'success_rate': health_score,\n",
    "        'avg_response_time': avg_response_time,\n",
    "        'health_score': composite_score\n",
    "    }\n",
    "\n",
    "# Create the dashboard\n",
    "test_df, summary_stats = create_analytics_dashboard()\n",
    "\n",
    "# Display comprehensive summary\n",
    "if summary_stats:\n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; margin: 20px 0;\">\n",
    "        <h2 style=\"margin-top: 0; text-align: center;\">üéØ HR Resume Search MCP API Test Summary</h2>\n",
    "        \n",
    "        <div style=\"display: grid; grid-template-columns: repeat(5, 1fr); gap: 20px; margin: 20px 0;\">\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold;\">{summary_stats['total_tests']}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Total Tests</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #4CAF50;\">{summary_stats['successful_tests']}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Successful</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #2196F3;\">{summary_stats['success_rate']:.1f}%</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Success Rate</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #FF9800;\">{summary_stats['avg_response_time']:.3f}s</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Avg Response</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <div style=\"font-size: 2.5em; font-weight: bold; color: #9C27B0;\">{summary_stats['health_score']:.0f}</div>\n",
    "                <div style=\"font-size: 0.9em; opacity: 0.9;\">Health Score</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"text-align: center; margin-top: 20px; padding: 15px; background: rgba(255,255,255,0.1); border-radius: 10px;\">\n",
    "            <h3 style=\"margin: 0;\">üìä Test Categories Covered</h3>\n",
    "            <p style=\"margin: 10px 0;\">Health Checks ‚Ä¢ Authentication ‚Ä¢ Resume Upload ‚Ä¢ Search Functionality ‚Ä¢ Performance Testing</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(summary_html))\n",
    "\n",
    "print(f\"\\nüìä Analytics dashboard created successfully!\")\n",
    "print(f\"üìà Total tests executed: {summary_stats['total_tests'] if summary_stats else 0}\")\n",
    "print(f\"‚úÖ Success rate: {summary_stats['success_rate']:.1f}% \" if summary_stats else \"No summary available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Test Report Generation\n",
    "\n",
    "Generate comprehensive test reports for documentation and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Generating Comprehensive Test Report...\")\n",
    "\n",
    "def generate_detailed_report():\n",
    "    \"\"\"Generate a detailed test report with all metrics\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Get client metrics\n",
    "    client_metrics = api_client.get_metrics()\n",
    "    \n",
    "    # Calculate detailed statistics\n",
    "    total_tests = sum(len(results) for results in all_test_results.values())\n",
    "    total_successful = sum(\n",
    "        sum(1 for r in results if r.get('success', False))\n",
    "        for results in all_test_results.values()\n",
    "    )\n",
    "    \n",
    "    # Performance analysis\n",
    "    all_response_times = []\n",
    "    for results in all_test_results.values():\n",
    "        for r in results:\n",
    "            if 'response_time' in r and r['response_time'] is not None:\n",
    "                all_response_times.append(r['response_time'])\n",
    "    \n",
    "    perf_stats = {\n",
    "        'min_response_time': min(all_response_times) if all_response_times else 0,\n",
    "        'max_response_time': max(all_response_times) if all_response_times else 0,\n",
    "        'avg_response_time': sum(all_response_times) / len(all_response_times) if all_response_times else 0,\n",
    "        'median_response_time': np.median(all_response_times) if all_response_times else 0,\n",
    "        'p95_response_time': np.percentile(all_response_times, 95) if all_response_times else 0,\n",
    "        'p99_response_time': np.percentile(all_response_times, 99) if all_response_times else 0\n",
    "    }\n",
    "    \n",
    "    # Generate detailed report\n",
    "    report = f\"\"\"\n",
    "# HR Resume Search MCP API - Test Report\n",
    "\n",
    "**Generated**: {timestamp}  \n",
    "**API Base URL**: {API_BASE_URL}  \n",
    "**Test Environment**: {os.getenv('ENVIRONMENT', 'development')}  \n",
    "\n",
    "## üìä Executive Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Tests Executed | {total_tests} |\n",
    "| Successful Tests | {total_successful} |\n",
    "| Success Rate | {(total_successful/total_tests*100):.1f}% |\n",
    "| Average Response Time | {perf_stats['avg_response_time']:.3f}s |\n",
    "| 95th Percentile Response Time | {perf_stats['p95_response_time']:.3f}s |\n",
    "| Tests Under 2s Response | {sum(1 for t in all_response_times if t < 2.0)/len(all_response_times)*100:.1f}% |\n",
    "\n",
    "## üéØ Test Categories\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add category details\n",
    "    for category, results in all_test_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        successful = sum(1 for r in results if r.get('success', False))\n",
    "        success_rate = (successful / len(results)) * 100\n",
    "        \n",
    "        # Calculate category response times\n",
    "        cat_response_times = [r['response_time'] for r in results \n",
    "                             if 'response_time' in r and r['response_time'] is not None]\n",
    "        avg_response = sum(cat_response_times) / len(cat_response_times) if cat_response_times else 0\n",
    "        \n",
    "        report += f\"\"\"\n",
    "### {category}\n",
    "\n",
    "- **Tests**: {len(results)}\n",
    "- **Success Rate**: {success_rate:.1f}%\n",
    "- **Average Response Time**: {avg_response:.3f}s\n",
    "- **Status**: {'‚úÖ Passing' if success_rate >= 80 else '‚ö†Ô∏è Needs Attention' if success_rate >= 50 else '‚ùå Failing'}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add failed tests\n",
    "        failed_tests = [r for r in results if not r.get('success', False)]\n",
    "        if failed_tests:\n",
    "            report += \"**Failed Tests:**\\n\"\n",
    "            for test in failed_tests[:5]:  # Show first 5 failures\n",
    "                test_name = test.get('test', 'Unknown test')\n",
    "                status_code = test.get('status_code', 'N/A')\n",
    "                error = test.get('error', 'No error details')\n",
    "                report += f\"- {test_name}: {status_code} - {error}\\n\"\n",
    "            \n",
    "            if len(failed_tests) > 5:\n",
    "                report += f\"- ... and {len(failed_tests) - 5} more failures\\n\"\n",
    "        \n",
    "        report += \"\\n\"\n",
    "    \n",
    "    # Add performance analysis\n",
    "    report += f\"\"\"\n",
    "## ‚ö° Performance Analysis\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Minimum Response Time | {perf_stats['min_response_time']:.3f}s |\n",
    "| Maximum Response Time | {perf_stats['max_response_time']:.3f}s |\n",
    "| Average Response Time | {perf_stats['avg_response_time']:.3f}s |\n",
    "| Median Response Time | {perf_stats['median_response_time']:.3f}s |\n",
    "| 95th Percentile | {perf_stats['p95_response_time']:.3f}s |\n",
    "| 99th Percentile | {perf_stats['p99_response_time']:.3f}s |\n",
    "\n",
    "### Performance Recommendations\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add performance recommendations\n",
    "    if perf_stats['avg_response_time'] > 2.0:\n",
    "        report += \"‚ùå **High Response Times**: Average response time exceeds 2s target\\n\"\n",
    "    elif perf_stats['avg_response_time'] > 1.0:\n",
    "        report += \"‚ö†Ô∏è **Moderate Response Times**: Consider optimization for better performance\\n\"\n",
    "    else:\n",
    "        report += \"‚úÖ **Good Performance**: Response times within acceptable range\\n\"\n",
    "    \n",
    "    if perf_stats['p95_response_time'] > 5.0:\n",
    "        report += \"‚ùå **Poor P95 Performance**: 95th percentile responses are too slow\\n\"\n",
    "    \n",
    "    # Add client metrics\n",
    "    report += f\"\"\"\n",
    "\n",
    "## üìà Client Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests Made | {client_metrics['requests_made']} |\n",
    "| Successful Requests | {client_metrics['requests_successful']} |\n",
    "| Failed Requests | {client_metrics['requests_failed']} |\n",
    "| Client Success Rate | {client_metrics.get('success_rate', 0):.1f}% |\n",
    "| Average Response Time | {client_metrics.get('average_response_time', 0):.3f}s |\n",
    "\n",
    "## üîç Detailed Test Results\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed test results table\n",
    "    report += \"| Test Category | Test Name | Status | Response Time | Status Code |\\n\"\n",
    "    report += \"|---------------|-----------|--------|---------------|-------------|\\n\"\n",
    "    \n",
    "    for category, results in all_test_results.items():\n",
    "        for result in results:\n",
    "            test_name = result.get('test', result.get('name', 'Unknown'))\n",
    "            status = '‚úÖ' if result.get('success', False) else '‚ùå'\n",
    "            response_time = f\"{result.get('response_time', 0):.3f}s\"\n",
    "            status_code = result.get('status_code', 'N/A')\n",
    "            \n",
    "            report += f\"| {category} | {test_name} | {status} | {response_time} | {status_code} |\\n\"\n",
    "    \n",
    "    # Add recommendations\n",
    "    report += f\"\"\"\n",
    "\n",
    "## üí° Recommendations\n",
    "\n",
    "### Immediate Actions\n",
    "\"\"\"\n",
    "    \n",
    "    if total_successful / total_tests < 0.8:\n",
    "        report += \"- üö® **Critical**: Success rate below 80% - investigate failing tests immediately\\n\"\n",
    "    \n",
    "    if perf_stats['avg_response_time'] > 2.0:\n",
    "        report += \"- ‚ö° **Performance**: Optimize response times - current average exceeds 2s target\\n\"\n",
    "    \n",
    "    # Check for specific issues\n",
    "    auth_tests = all_test_results.get('Authentication', [])\n",
    "    auth_success = sum(1 for r in auth_tests if r.get('success', False)) / len(auth_tests) if auth_tests else 1\n",
    "    \n",
    "    if auth_success < 0.8:\n",
    "        report += \"- üîê **Authentication**: Review authentication endpoints - low success rate\\n\"\n",
    "    \n",
    "    upload_tests = all_test_results.get('Resume Upload', [])\n",
    "    upload_success = sum(1 for r in upload_tests if r.get('success', False)) / len(upload_tests) if upload_tests else 1\n",
    "    \n",
    "    if upload_success < 0.8:\n",
    "        report += \"- üìÑ **File Upload**: File upload functionality needs attention\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "### Long-term Improvements\n",
    "- üìä **Monitoring**: Implement continuous performance monitoring\n",
    "- üîÑ **Automation**: Add these tests to CI/CD pipeline\n",
    "- üìà **Metrics**: Set up alerting for performance degradation\n",
    "- üß™ **Testing**: Expand test coverage for edge cases\n",
    "\n",
    "---\n",
    "\n",
    "**Report Generated by**: HR Resume Search MCP API Testing Suite  \n",
    "**Notebook**: `notebooks/api_testing.ipynb`  \n",
    "**Timestamp**: {timestamp}  \n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the report\n",
    "test_report = generate_detailed_report()\n",
    "\n",
    "# Save report to file\n",
    "report_file = f\"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(test_report)\n",
    "\n",
    "print(f\"‚úÖ Comprehensive test report saved to: {report_file}\")\n",
    "\n",
    "# Display report summary\n",
    "display(Markdown(\"## üìã Test Report Summary\\n\\n\" + test_report[:1000] + \"\\n\\n*Full report saved to file.*\"))\n",
    "\n",
    "# Clean up\n",
    "api_client.close()\n",
    "\n",
    "print(\"\\nüéâ API Testing Suite completed successfully!\")\n",
    "print(f\"üìä Total tests executed: {sum(len(results) for results in all_test_results.values())}\")\n",
    "print(f\"üìù Report saved: {report_file}\")\n",
    "print(f\"üßπ Resources cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary and Next Steps\n",
    "\n",
    "This comprehensive testing notebook has covered:\n",
    "\n",
    "### ‚úÖ Completed Tests\n",
    "1. **API Health Checks** - Verified basic connectivity and endpoint availability\n",
    "2. **Authentication Flow** - Tested login, token management, and protected endpoints\n",
    "3. **Resume Upload** - Tested file upload with various formats and Claude AI parsing\n",
    "4. **Search Functionality** - Comprehensive search testing with different query types\n",
    "5. **Performance Testing** - Load testing with concurrent requests and stress testing\n",
    "6. **Data Visualization** - Rich charts and analytics for all test results\n",
    "\n",
    "### üìä Key Features\n",
    "- **Interactive Testing** - Real-time API testing with immediate feedback\n",
    "- **Performance Metrics** - Detailed response time and success rate analysis\n",
    "- **Visual Analytics** - Comprehensive charts and dashboards\n",
    "- **Automated Reporting** - Generated markdown reports for documentation\n",
    "- **Error Handling** - Robust error detection and reporting\n",
    "\n",
    "### üöÄ Usage Instructions\n",
    "1. Ensure API server is running at `http://localhost:8000`\n",
    "2. Run notebook cells sequentially\n",
    "3. Monitor test results and visualizations\n",
    "4. Review generated test report\n",
    "5. Use insights for API improvement\n",
    "\n",
    "### üîß Customization\n",
    "- Modify `TEST_TIMEOUT` for different response time requirements\n",
    "- Adjust `CONCURRENT_REQUESTS` for different load testing scenarios\n",
    "- Update test data in helper functions for specific testing needs\n",
    "- Extend visualization functions for custom charts\n",
    "\n",
    "### üìà Next Steps\n",
    "- Integrate with CI/CD pipeline for automated testing\n",
    "- Add more sophisticated test scenarios\n",
    "- Implement test data factories for larger datasets\n",
    "- Create alerts based on performance thresholds\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Testing! üß™‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}