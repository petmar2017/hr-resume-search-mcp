{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Performance Benchmarking Suite\n",
    "\n",
    "**Comprehensive performance analysis for HR Resume Search MCP API**\n",
    "\n",
    "## Benchmarking Features\n",
    "- ‚ö° **Response Time Analysis** - Detailed latency measurements\n",
    "- üî• **Concurrent Load Testing** - Multi-user simulation\n",
    "- üìà **Throughput Analysis** - Requests per second metrics\n",
    "- üéØ **Search Quality Evaluation** - Relevance and accuracy scoring\n",
    "- üìä **Resource Utilization** - CPU, memory, and database metrics\n",
    "- üîç **Bottleneck Identification** - Performance optimization insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import statistics\n",
    "import psutil\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# IPython display\n",
    "from IPython.display import display, HTML, JSON, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, IntSlider\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üìä Performance benchmarking suite initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')\n",
    "API_PREFIX = \"/api/v1\"\n",
    "API_URL = f\"{API_BASE_URL}{API_PREFIX}\"\n",
    "\n",
    "# Benchmarking configuration\n",
    "BENCHMARK_CONFIG = {\n",
    "    'warmup_requests': 10,\n",
    "    'measurement_duration': 60,  # seconds\n",
    "    'max_concurrent_users': 100,\n",
    "    'step_size': 10,\n",
    "    'request_timeout': 30,\n",
    "    'think_time': 1.0,  # seconds between requests\n",
    "    'error_threshold': 5.0,  # max acceptable error rate %\n",
    "    'response_time_threshold': 2.0,  # max acceptable response time in seconds\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "config_display = f\"\"\"\n",
    "<div style=\"background: linear-gradient(135deg, #ff6b6b 0%, #feca57 50%, #48dbfb 100%); color: white; padding: 20px; border-radius: 15px; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; text-align: center;\">‚ö° Performance Benchmarking Configuration</h3>\n",
    "    <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 15px 0;\">\n",
    "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "            <h4>üéØ Target API</h4>\n",
    "            <p><strong>URL:</strong> {API_URL}</p>\n",
    "            <p><strong>Timeout:</strong> {BENCHMARK_CONFIG['request_timeout']}s</p>\n",
    "        </div>\n",
    "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "            <h4>üî• Load Testing</h4>\n",
    "            <p><strong>Max Concurrent:</strong> {BENCHMARK_CONFIG['max_concurrent_users']}</p>\n",
    "            <p><strong>Duration:</strong> {BENCHMARK_CONFIG['measurement_duration']}s</p>\n",
    "        </div>\n",
    "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "            <h4>üìä Quality Metrics</h4>\n",
    "            <p><strong>Error Threshold:</strong> {BENCHMARK_CONFIG['error_threshold']}%</p>\n",
    "            <p><strong>Response Time:</strong> {BENCHMARK_CONFIG['response_time_threshold']}s</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(config_display))\n",
    "print(\"‚öôÔ∏è Benchmarking configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Performance Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetric:\n",
    "    \"\"\"Individual performance measurement\"\"\"\n",
    "    request_id: str\n",
    "    endpoint: str\n",
    "    method: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    response_time: float\n",
    "    status_code: int\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    response_size: int = 0\n",
    "    user_id: int = 0\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "@dataclass\n",
    "class SystemMetrics:\n",
    "    \"\"\"System resource metrics\"\"\"\n",
    "    timestamp: datetime\n",
    "    cpu_percent: float\n",
    "    memory_percent: float\n",
    "    memory_used_mb: float\n",
    "    disk_io_read: float\n",
    "    disk_io_write: float\n",
    "    network_sent: float\n",
    "    network_recv: float\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Complete benchmark test result\"\"\"\n",
    "    test_name: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    duration: float\n",
    "    metrics: List[PerformanceMetric]\n",
    "    system_metrics: List[SystemMetrics]\n",
    "    summary_stats: Dict[str, Any]\n",
    "    \n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Comprehensive performance benchmarking framework\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = API_URL):\n",
    "        self.api_url = api_url\n",
    "        self.session = None\n",
    "        self.metrics: List[PerformanceMetric] = []\n",
    "        self.system_metrics: List[SystemMetrics] = []\n",
    "        self.monitoring_active = False\n",
    "        \n",
    "    async def create_session(self):\n",
    "        \"\"\"Create aiohttp session with optimized settings\"\"\"\n",
    "        connector = aiohttp.TCPConnector(\n",
    "            limit=200,\n",
    "            limit_per_host=50,\n",
    "            keepalive_timeout=30,\n",
    "            enable_cleanup_closed=True\n",
    "        )\n",
    "        \n",
    "        timeout = aiohttp.ClientTimeout(total=BENCHMARK_CONFIG['request_timeout'])\n",
    "        \n",
    "        self.session = aiohttp.ClientSession(\n",
    "            connector=connector,\n",
    "            timeout=timeout,\n",
    "            headers={'User-Agent': 'PerformanceBenchmark/1.0'}\n",
    "        )\n",
    "    \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close aiohttp session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "    \n",
    "    def start_system_monitoring(self):\n",
    "        \"\"\"Start system resource monitoring\"\"\"\n",
    "        self.monitoring_active = True\n",
    "        \n",
    "        def monitor():\n",
    "            while self.monitoring_active:\n",
    "                try:\n",
    "                    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "                    memory = psutil.virtual_memory()\n",
    "                    disk_io = psutil.disk_io_counters()\n",
    "                    network_io = psutil.net_io_counters()\n",
    "                    \n",
    "                    metric = SystemMetrics(\n",
    "                        timestamp=datetime.now(),\n",
    "                        cpu_percent=cpu_percent,\n",
    "                        memory_percent=memory.percent,\n",
    "                        memory_used_mb=memory.used / (1024 * 1024),\n",
    "                        disk_io_read=disk_io.read_bytes if disk_io else 0,\n",
    "                        disk_io_write=disk_io.write_bytes if disk_io else 0,\n",
    "                        network_sent=network_io.bytes_sent if network_io else 0,\n",
    "                        network_recv=network_io.bytes_recv if network_io else 0\n",
    "                    )\n",
    "                    \n",
    "                    self.system_metrics.append(metric)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Monitoring error: {e}\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        self.monitor_thread = threading.Thread(target=monitor, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "    \n",
    "    def stop_system_monitoring(self):\n",
    "        \"\"\"Stop system resource monitoring\"\"\"\n",
    "        self.monitoring_active = False\n",
    "    \n",
    "    async def make_request(\n",
    "        self, \n",
    "        endpoint: str, \n",
    "        method: str = 'GET', \n",
    "        data: Optional[Dict] = None,\n",
    "        user_id: int = 0\n",
    "    ) -> PerformanceMetric:\n",
    "        \"\"\"Make a single HTTP request and measure performance\"\"\"\n",
    "        \n",
    "        request_id = str(uuid.uuid4())[:8]\n",
    "        url = f\"{self.api_url}{endpoint}\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            async with self.session.request(\n",
    "                method=method,\n",
    "                url=url,\n",
    "                json=data if method in ['POST', 'PUT', 'PATCH'] else None,\n",
    "                params=data if method == 'GET' else None\n",
    "            ) as response:\n",
    "                \n",
    "                end_time = time.time()\n",
    "                response_time = end_time - start_time\n",
    "                \n",
    "                # Try to read response body for size calculation\n",
    "                try:\n",
    "                    response_text = await response.text()\n",
    "                    response_size = len(response_text.encode('utf-8'))\n",
    "                except:\n",
    "                    response_size = 0\n",
    "                \n",
    "                metric = PerformanceMetric(\n",
    "                    request_id=request_id,\n",
    "                    endpoint=endpoint,\n",
    "                    method=method,\n",
    "                    start_time=start_time,\n",
    "                    end_time=end_time,\n",
    "                    response_time=response_time,\n",
    "                    status_code=response.status,\n",
    "                    success=200 <= response.status < 400,\n",
    "                    response_size=response_size,\n",
    "                    user_id=user_id\n",
    "                )\n",
    "                \n",
    "                return metric\n",
    "                \n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            metric = PerformanceMetric(\n",
    "                request_id=request_id,\n",
    "                endpoint=endpoint,\n",
    "                method=method,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                response_time=response_time,\n",
    "                status_code=0,\n",
    "                success=False,\n",
    "                error_message=str(e),\n",
    "                user_id=user_id\n",
    "            )\n",
    "            \n",
    "            return metric\n",
    "    \n",
    "    async def warmup(self, endpoints: List[str], requests_per_endpoint: int = 5):\n",
    "        \"\"\"Warmup API with initial requests\"\"\"\n",
    "        print(f\"üî• Warming up API with {requests_per_endpoint} requests per endpoint...\")\n",
    "        \n",
    "        warmup_metrics = []\n",
    "        \n",
    "        for endpoint in endpoints:\n",
    "            for i in range(requests_per_endpoint):\n",
    "                metric = await self.make_request(endpoint)\n",
    "                warmup_metrics.append(metric)\n",
    "                await asyncio.sleep(0.1)  # Small delay\n",
    "        \n",
    "        success_rate = sum(1 for m in warmup_metrics if m.success) / len(warmup_metrics) * 100\n",
    "        avg_response_time = statistics.mean(m.response_time for m in warmup_metrics)\n",
    "        \n",
    "        print(f\"‚úÖ Warmup completed: {success_rate:.1f}% success rate, {avg_response_time:.3f}s avg response time\")\n",
    "        \n",
    "        return warmup_metrics\n",
    "    \n",
    "    def calculate_statistics(self, metrics: List[PerformanceMetric]) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive performance statistics\"\"\"\n",
    "        if not metrics:\n",
    "            return {}\n",
    "        \n",
    "        successful_requests = [m for m in metrics if m.success]\n",
    "        failed_requests = [m for m in metrics if not m.success]\n",
    "        \n",
    "        response_times = [m.response_time for m in successful_requests]\n",
    "        \n",
    "        total_duration = max(m.end_time for m in metrics) - min(m.start_time for m in metrics)\n",
    "        \n",
    "        stats = {\n",
    "            'total_requests': len(metrics),\n",
    "            'successful_requests': len(successful_requests),\n",
    "            'failed_requests': len(failed_requests),\n",
    "            'success_rate': len(successful_requests) / len(metrics) * 100,\n",
    "            'total_duration': total_duration,\n",
    "            'requests_per_second': len(metrics) / total_duration if total_duration > 0 else 0,\n",
    "            'successful_rps': len(successful_requests) / total_duration if total_duration > 0 else 0\n",
    "        }\n",
    "        \n",
    "        if response_times:\n",
    "            stats.update({\n",
    "                'avg_response_time': statistics.mean(response_times),\n",
    "                'median_response_time': statistics.median(response_times),\n",
    "                'min_response_time': min(response_times),\n",
    "                'max_response_time': max(response_times),\n",
    "                'p90_response_time': np.percentile(response_times, 90),\n",
    "                'p95_response_time': np.percentile(response_times, 95),\n",
    "                'p99_response_time': np.percentile(response_times, 99),\n",
    "                'std_response_time': statistics.stdev(response_times) if len(response_times) > 1 else 0\n",
    "            })\n",
    "        \n",
    "        # Response size statistics\n",
    "        response_sizes = [m.response_size for m in successful_requests if m.response_size > 0]\n",
    "        if response_sizes:\n",
    "            stats.update({\n",
    "                'avg_response_size': statistics.mean(response_sizes),\n",
    "                'total_bytes_transferred': sum(response_sizes)\n",
    "            })\n",
    "        \n",
    "        # Error analysis\n",
    "        error_types = {}\n",
    "        for request in failed_requests:\n",
    "            error_key = f\"HTTP_{request.status_code}\" if request.status_code > 0 else \"network_error\"\n",
    "            error_types[error_key] = error_types.get(error_key, 0) + 1\n",
    "        \n",
    "        stats['error_breakdown'] = error_types\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize benchmark framework\n",
    "benchmark = PerformanceBenchmark()\n",
    "print(\"üèÅ Performance benchmark framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Benchmark Test 1: Response Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_response_time_benchmark():\n",
    "    \"\"\"Comprehensive response time analysis across all endpoints\"\"\"\n",
    "    \n",
    "    print(\"‚ö° Starting Response Time Analysis...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define test endpoints with different complexity levels\n",
    "    test_endpoints = [\n",
    "        {\n",
    "            'endpoint': '/health',\n",
    "            'method': 'GET',\n",
    "            'complexity': 'simple',\n",
    "            'description': 'Health check endpoint'\n",
    "        },\n",
    "        {\n",
    "            'endpoint': '/search/candidates',\n",
    "            'method': 'POST',\n",
    "            'complexity': 'medium',\n",
    "            'description': 'Basic candidate search',\n",
    "            'data': {\n",
    "                'query': 'software engineer',\n",
    "                'limit': 10\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'endpoint': '/search/candidates',\n",
    "            'method': 'POST',\n",
    "            'complexity': 'complex',\n",
    "            'description': 'Advanced candidate search with filters',\n",
    "            'data': {\n",
    "                'query': 'senior python developer',\n",
    "                'filters': {\n",
    "                    'skills': ['Python', 'Django', 'PostgreSQL'],\n",
    "                    'experience_years': {'min': 5, 'max': 10},\n",
    "                    'departments': ['Engineering']\n",
    "                },\n",
    "                'scoring': {\n",
    "                    'skills_weight': 0.4,\n",
    "                    'experience_weight': 0.3,\n",
    "                    'title_weight': 0.3\n",
    "                },\n",
    "                'include_score_breakdown': True,\n",
    "                'limit': 20\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'endpoint': '/search/smart',\n",
    "            'method': 'POST',\n",
    "            'complexity': 'complex',\n",
    "            'description': 'AI-powered smart search',\n",
    "            'data': {\n",
    "                'query': 'Find me data scientists with machine learning experience in fintech companies',\n",
    "                'enhance_query': True,\n",
    "                'explain_reasoning': True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    await benchmark.create_session()\n",
    "    \n",
    "    try:\n",
    "        # Warmup\n",
    "        warmup_endpoints = [ep['endpoint'] for ep in test_endpoints]\n",
    "        await benchmark.warmup(warmup_endpoints, 3)\n",
    "        \n",
    "        # Start system monitoring\n",
    "        benchmark.start_system_monitoring()\n",
    "        \n",
    "        # Run response time tests\n",
    "        all_metrics = []\n",
    "        requests_per_endpoint = 20\n",
    "        \n",
    "        for endpoint_config in test_endpoints:\n",
    "            print(f\"\\nüîç Testing {endpoint_config['description']}...\")\n",
    "            endpoint_metrics = []\n",
    "            \n",
    "            for i in range(requests_per_endpoint):\n",
    "                metric = await benchmark.make_request(\n",
    "                    endpoint=endpoint_config['endpoint'],\n",
    "                    method=endpoint_config['method'],\n",
    "                    data=endpoint_config.get('data')\n",
    "                )\n",
    "                \n",
    "                endpoint_metrics.append(metric)\n",
    "                all_metrics.append(metric)\n",
    "                \n",
    "                # Small delay between requests\n",
    "                await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Calculate endpoint-specific statistics\n",
    "            endpoint_stats = benchmark.calculate_statistics(endpoint_metrics)\n",
    "            \n",
    "            print(f\"   ‚úÖ Completed: {endpoint_stats['success_rate']:.1f}% success rate\")\n",
    "            print(f\"   ‚è±Ô∏è Avg response time: {endpoint_stats.get('avg_response_time', 0):.3f}s\")\n",
    "            print(f\"   üìä P95 response time: {endpoint_stats.get('p95_response_time', 0):.3f}s\")\n",
    "        \n",
    "        # Stop monitoring\n",
    "        benchmark.stop_system_monitoring()\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        overall_stats = benchmark.calculate_statistics(all_metrics)\n",
    "        \n",
    "        print(f\"\\nüìä Overall Response Time Analysis Results:\")\n",
    "        print(f\"   Total Requests: {overall_stats['total_requests']}\")\n",
    "        print(f\"   Success Rate: {overall_stats['success_rate']:.1f}%\")\n",
    "        print(f\"   Average Response Time: {overall_stats.get('avg_response_time', 0):.3f}s\")\n",
    "        print(f\"   P95 Response Time: {overall_stats.get('p95_response_time', 0):.3f}s\")\n",
    "        print(f\"   P99 Response Time: {overall_stats.get('p99_response_time', 0):.3f}s\")\n",
    "        \n",
    "        return all_metrics, overall_stats\n",
    "        \n",
    "    finally:\n",
    "        await benchmark.close_session()\n",
    "\n",
    "# Run the response time benchmark\n",
    "response_time_metrics, response_time_stats = await run_response_time_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize response time analysis results\n",
    "def visualize_response_time_analysis(metrics: List[PerformanceMetric], stats: Dict[str, Any]):\n",
    "    \"\"\"Create comprehensive response time visualizations\"\"\"\n",
    "    \n",
    "    # Convert metrics to DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'endpoint': m.endpoint,\n",
    "        'method': m.method,\n",
    "        'response_time': m.response_time,\n",
    "        'success': m.success,\n",
    "        'status_code': m.status_code,\n",
    "        'response_size': m.response_size,\n",
    "        'timestamp': m.timestamp\n",
    "    } for m in metrics])\n",
    "    \n",
    "    # Create subplot dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Response Time Distribution',\n",
    "            'Response Time by Endpoint',\n",
    "            'Response Time Timeline',\n",
    "            'Success Rate by Endpoint',\n",
    "            'Response Size vs Time',\n",
    "            'Performance Summary'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"type\": \"indicator\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Response time distribution\n",
    "    successful_times = df[df['success']]['response_time']\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=successful_times,\n",
    "            nbinsx=30,\n",
    "            name=\"Response Time Distribution\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Response time by endpoint (box plot)\n",
    "    endpoints = df['endpoint'].unique()\n",
    "    for endpoint in endpoints:\n",
    "        endpoint_data = df[df['endpoint'] == endpoint]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=endpoint_data['response_time'],\n",
    "                name=endpoint.split('/')[-1],\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Response time timeline\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df['response_time'],\n",
    "            mode='lines+markers',\n",
    "            name=\"Response Time Timeline\",\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Success rate by endpoint\n",
    "    success_rates = df.groupby('endpoint')['success'].mean() * 100\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[ep.split('/')[-1] for ep in success_rates.index],\n",
    "            y=success_rates.values,\n",
    "            name=\"Success Rate\",\n",
    "            marker_color='green'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Response size vs time\n",
    "    size_data = df[df['response_size'] > 0]\n",
    "    if not size_data.empty:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=size_data['response_time'],\n",
    "                y=size_data['response_size'],\n",
    "                mode='markers',\n",
    "                name=\"Size vs Time\",\n",
    "                marker=dict(size=8, opacity=0.6)\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Performance indicator\n",
    "    performance_score = min(100, (stats.get('success_rate', 0) * 0.6 + \n",
    "                                 max(0, (2.0 - stats.get('avg_response_time', 2.0)) * 50) * 0.4))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=performance_score,\n",
    "            title={'text': \"Performance Score\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [None, 100]},\n",
    "                'bar': {'color': \"darkblue\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 60], 'color': \"lightgray\"},\n",
    "                    {'range': [60, 80], 'color': \"yellow\"},\n",
    "                    {'range': [80, 100], 'color': \"green\"}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 90\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        title_text=\"Response Time Analysis Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Display detailed statistics table\n",
    "    stats_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 15px 0;\">\n",
    "        <h4>üìä Detailed Response Time Statistics</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px;\">\n",
    "            <div style=\"text-align: center; padding: 15px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #007acc;\">{stats.get('avg_response_time', 0):.3f}s</div>\n",
    "                <div>Average Response Time</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 15px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #28a745;\">{stats.get('p95_response_time', 0):.3f}s</div>\n",
    "                <div>95th Percentile</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 15px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #17a2b8;\">{stats.get('p99_response_time', 0):.3f}s</div>\n",
    "                <div>99th Percentile</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; padding: 15px; background: white; border-radius: 5px;\">\n",
    "                <div style=\"font-size: 24px; font-weight: bold; color: #dc3545;\">{stats.get('success_rate', 0):.1f}%</div>\n",
    "                <div>Success Rate</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 20px;\">\n",
    "            <h5>Performance Thresholds Analysis:</h5>\n",
    "            <ul>\n",
    "                <li><strong>Target Response Time:</strong> < {BENCHMARK_CONFIG['response_time_threshold']}s \n",
    "                    {'‚úÖ PASS' if stats.get('avg_response_time', 999) < BENCHMARK_CONFIG['response_time_threshold'] else '‚ùå FAIL'}</li>\n",
    "                <li><strong>Target Error Rate:</strong> < {BENCHMARK_CONFIG['error_threshold']}% \n",
    "                    {'‚úÖ PASS' if (100 - stats.get('success_rate', 0)) < BENCHMARK_CONFIG['error_threshold'] else '‚ùå FAIL'}</li>\n",
    "                <li><strong>P95 Response Time:</strong> {stats.get('p95_response_time', 0):.3f}s \n",
    "                    {'‚úÖ GOOD' if stats.get('p95_response_time', 999) < BENCHMARK_CONFIG['response_time_threshold'] * 1.5 else '‚ö†Ô∏è NEEDS ATTENTION'}</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(stats_html))\n",
    "\n",
    "# Visualize the response time analysis\n",
    "if response_time_metrics:\n",
    "    visualize_response_time_analysis(response_time_metrics, response_time_stats)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No response time data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Benchmark Test 2: Concurrent Load Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_concurrent_load_test():\n",
    "    \"\"\"Comprehensive concurrent load testing with multiple user simulation\"\"\"\n",
    "    \n",
    "    print(\"üî• Starting Concurrent Load Testing...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    await benchmark.create_session()\n",
    "    \n",
    "    try:\n",
    "        # Test configuration\n",
    "        concurrent_users = [1, 5, 10, 20, 30, 50]  # Progressive load\n",
    "        test_duration = 30  # seconds per load level\n",
    "        endpoint = '/search/candidates'\n",
    "        \n",
    "        # Test payload\n",
    "        test_payload = {\n",
    "            'query': 'software engineer',\n",
    "            'filters': {\n",
    "                'skills': ['Python', 'JavaScript'],\n",
    "                'experience_years': {'min': 2}\n",
    "            },\n",
    "            'limit': 10\n",
    "        }\n",
    "        \n",
    "        load_test_results = []\n",
    "        \n",
    "        # Start system monitoring\n",
    "        benchmark.start_system_monitoring()\n",
    "        \n",
    "        for user_count in concurrent_users:\n",
    "            print(f\"\\nüîÑ Testing with {user_count} concurrent users...\")\n",
    "            \n",
    "            # Reset metrics for this load level\n",
    "            level_metrics = []\n",
    "            \n",
    "            async def user_simulation(user_id: int):\n",
    "                \"\"\"Simulate a single user's behavior\"\"\"\n",
    "                user_metrics = []\n",
    "                end_time = time.time() + test_duration\n",
    "                \n",
    "                while time.time() < end_time:\n",
    "                    # Make request\n",
    "                    metric = await benchmark.make_request(\n",
    "                        endpoint=endpoint,\n",
    "                        method='POST',\n",
    "                        data=test_payload,\n",
    "                        user_id=user_id\n",
    "                    )\n",
    "                    \n",
    "                    user_metrics.append(metric)\n",
    "                    \n",
    "                    # Think time between requests\n",
    "                    await asyncio.sleep(BENCHMARK_CONFIG['think_time'])\n",
    "                \n",
    "                return user_metrics\n",
    "            \n",
    "            # Run concurrent users\n",
    "            start_time = time.time()\n",
    "            \n",
    "            user_tasks = [\n",
    "                user_simulation(user_id) \n",
    "                for user_id in range(user_count)\n",
    "            ]\n",
    "            \n",
    "            user_results = await asyncio.gather(*user_tasks)\n",
    "            \n",
    "            # Flatten results\n",
    "            for user_metrics in user_results:\n",
    "                level_metrics.extend(user_metrics)\n",
    "            \n",
    "            # Calculate statistics for this load level\n",
    "            level_stats = benchmark.calculate_statistics(level_metrics)\n",
    "            level_stats['concurrent_users'] = user_count\n",
    "            level_stats['test_duration'] = test_duration\n",
    "            \n",
    "            load_test_results.append({\n",
    "                'concurrent_users': user_count,\n",
    "                'metrics': level_metrics,\n",
    "                'stats': level_stats\n",
    "            })\n",
    "            \n",
    "            # Display results for this level\n",
    "            print(f\"   ‚úÖ Completed: {level_stats['success_rate']:.1f}% success rate\")\n",
    "            print(f\"   üìà Throughput: {level_stats['successful_rps']:.1f} req/s\")\n",
    "            print(f\"   ‚è±Ô∏è Avg response time: {level_stats.get('avg_response_time', 0):.3f}s\")\n",
    "            print(f\"   üìä P95 response time: {level_stats.get('p95_response_time', 0):.3f}s\")\n",
    "            \n",
    "            # Short break between load levels\n",
    "            await asyncio.sleep(2)\n",
    "        \n",
    "        # Stop monitoring\n",
    "        benchmark.stop_system_monitoring()\n",
    "        \n",
    "        print(f\"\\nüèÅ Concurrent Load Testing Completed!\")\n",
    "        \n",
    "        return load_test_results\n",
    "        \n",
    "    finally:\n",
    "        await benchmark.close_session()\n",
    "\n",
    "# Run concurrent load test\n",
    "load_test_results = await run_concurrent_load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize concurrent load test results\n",
    "def visualize_load_test_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Create comprehensive load test visualizations\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No load test data available\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for visualization\n",
    "    user_counts = [r['concurrent_users'] for r in results]\n",
    "    success_rates = [r['stats']['success_rate'] for r in results]\n",
    "    throughputs = [r['stats']['successful_rps'] for r in results]\n",
    "    avg_response_times = [r['stats'].get('avg_response_time', 0) for r in results]\n",
    "    p95_response_times = [r['stats'].get('p95_response_time', 0) for r in results]\n",
    "    \n",
    "    # Create load test dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Throughput vs Concurrent Users',\n",
    "            'Response Time vs Load',\n",
    "            'Success Rate vs Load',\n",
    "            'Performance Degradation'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": True}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Throughput vs concurrent users\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=throughputs,\n",
    "            mode='lines+markers',\n",
    "            name='Throughput (req/s)',\n",
    "            line=dict(color='green', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Response time vs load\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=avg_response_times,\n",
    "            mode='lines+markers',\n",
    "            name='Avg Response Time',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=6)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=p95_response_times,\n",
    "            mode='lines+markers',\n",
    "            name='P95 Response Time',\n",
    "            line=dict(color='red', width=2, dash='dash'),\n",
    "            marker=dict(size=6)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Success rate vs load\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=success_rates,\n",
    "            mode='lines+markers',\n",
    "            name='Success Rate (%)',\n",
    "            line=dict(color='orange', width=3),\n",
    "            marker=dict(size=8),\n",
    "            fill='tonexty' if len(success_rates) > 1 else None\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Performance degradation (throughput vs response time)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=throughputs,\n",
    "            mode='lines+markers',\n",
    "            name='Throughput',\n",
    "            line=dict(color='green'),\n",
    "            yaxis='y'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=user_counts,\n",
    "            y=avg_response_times,\n",
    "            mode='lines+markers',\n",
    "            name='Response Time',\n",
    "            line=dict(color='red'),\n",
    "            yaxis='y2'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Concurrent Load Testing Results\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axis labels\n",
    "    fig.update_xaxes(title_text=\"Concurrent Users\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Concurrent Users\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Concurrent Users\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Concurrent Users\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Requests/Second\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Response Time (s)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Throughput\", row=2, col=2)\n",
    "    \n",
    "    # Add secondary y-axis for last subplot\n",
    "    fig.update_layout(yaxis4=dict(title=\"Response Time (s)\", overlaying=\"y3\", side=\"right\"))\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Find performance breaking point\n",
    "    breaking_point = None\n",
    "    for i, result in enumerate(results):\n",
    "        if (result['stats']['success_rate'] < 95 or \n",
    "            result['stats'].get('avg_response_time', 0) > BENCHMARK_CONFIG['response_time_threshold']):\n",
    "            breaking_point = result['concurrent_users']\n",
    "            break\n",
    "    \n",
    "    # Calculate peak performance\n",
    "    peak_throughput = max(throughputs)\n",
    "    peak_users = user_counts[throughputs.index(peak_throughput)]\n",
    "    \n",
    "    # Display summary\n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 15px; margin: 15px 0;\">\n",
    "        <h4 style=\"margin-top: 0;\">üî• Concurrent Load Test Summary</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 15px 0;\">\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>üèÜ Peak Performance</h5>\n",
    "                <p><strong>Throughput:</strong> {peak_throughput:.1f} req/s</p>\n",
    "                <p><strong>At:</strong> {peak_users} concurrent users</p>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>‚ö†Ô∏è Breaking Point</h5>\n",
    "                <p><strong>Users:</strong> {breaking_point or 'Not reached'}</p>\n",
    "                <p><strong>Reason:</strong> {'Performance degradation' if breaking_point else 'Stable throughout test'}</p>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>üìä Final Results</h5>\n",
    "                <p><strong>Max Users Tested:</strong> {max(user_counts)}</p>\n",
    "                <p><strong>Final Success Rate:</strong> {success_rates[-1]:.1f}%</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(summary_html))\n",
    "\n",
    "# Visualize load test results\n",
    "if load_test_results:\n",
    "    visualize_load_test_results(load_test_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No load test data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Search Quality Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQualityEvaluator:\n",
    "    \"\"\"Framework for evaluating search result quality and relevance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_queries = [\n",
    "            {\n",
    "                'query': 'Python developer with 5 years experience',\n",
    "                'expected_skills': ['Python'],\n",
    "                'expected_experience_min': 4,\n",
    "                'expected_departments': ['Engineering', 'Product'],\n",
    "                'relevance_threshold': 0.7\n",
    "            },\n",
    "            {\n",
    "                'query': 'Senior data scientist machine learning fintech',\n",
    "                'expected_skills': ['Python', 'Machine Learning', 'Data Science'],\n",
    "                'expected_industries': ['fintech', 'finance'],\n",
    "                'expected_seniority': ['Senior', 'Lead', 'Principal'],\n",
    "                'relevance_threshold': 0.8\n",
    "            },\n",
    "            {\n",
    "                'query': 'Frontend engineer React JavaScript',\n",
    "                'expected_skills': ['React', 'JavaScript'],\n",
    "                'expected_roles': ['Frontend', 'UI', 'Web'],\n",
    "                'relevance_threshold': 0.75\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def evaluate_skill_relevance(self, candidate: Dict, expected_skills: List[str]) -> float:\n",
    "        \"\"\"Evaluate how well candidate skills match expected skills\"\"\"\n",
    "        candidate_skills = [skill.lower() for skill in candidate.get('skills', [])]\n",
    "        expected_skills_lower = [skill.lower() for skill in expected_skills]\n",
    "        \n",
    "        matches = sum(1 for skill in expected_skills_lower if any(skill in cs for cs in candidate_skills))\n",
    "        return matches / len(expected_skills) if expected_skills else 0\n",
    "    \n",
    "    def evaluate_experience_relevance(self, candidate: Dict, min_years: int) -> float:\n",
    "        \"\"\"Evaluate experience level match\"\"\"\n",
    "        years_exp = candidate.get('metadata', {}).get('years_experience', 0)\n",
    "        if years_exp >= min_years:\n",
    "            return min(1.0, years_exp / (min_years * 1.5))  # Scale appropriately\n",
    "        return years_exp / min_years if min_years > 0 else 0\n",
    "    \n",
    "    def evaluate_department_relevance(self, candidate: Dict, expected_departments: List[str]) -> float:\n",
    "        \"\"\"Evaluate department/role relevance\"\"\"\n",
    "        experience = candidate.get('experience', [])\n",
    "        if not experience:\n",
    "            return 0\n",
    "        \n",
    "        current_dept = experience[0].get('department', '').lower()\n",
    "        expected_lower = [dept.lower() for dept in expected_departments]\n",
    "        \n",
    "        return 1.0 if current_dept in expected_lower else 0.5\n",
    "    \n",
    "    def calculate_relevance_score(self, candidate: Dict, query_criteria: Dict) -> float:\n",
    "        \"\"\"Calculate overall relevance score for a candidate\"\"\"\n",
    "        scores = []\n",
    "        weights = []\n",
    "        \n",
    "        # Skills relevance\n",
    "        if 'expected_skills' in query_criteria:\n",
    "            skill_score = self.evaluate_skill_relevance(candidate, query_criteria['expected_skills'])\n",
    "            scores.append(skill_score)\n",
    "            weights.append(0.4)\n",
    "        \n",
    "        # Experience relevance\n",
    "        if 'expected_experience_min' in query_criteria:\n",
    "            exp_score = self.evaluate_experience_relevance(candidate, query_criteria['expected_experience_min'])\n",
    "            scores.append(exp_score)\n",
    "            weights.append(0.3)\n",
    "        \n",
    "        # Department relevance\n",
    "        if 'expected_departments' in query_criteria:\n",
    "            dept_score = self.evaluate_department_relevance(candidate, query_criteria['expected_departments'])\n",
    "            scores.append(dept_score)\n",
    "            weights.append(0.3)\n",
    "        \n",
    "        # Weighted average\n",
    "        if scores and weights:\n",
    "            return sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)\n",
    "        return 0\n",
    "    \n",
    "    async def evaluate_search_quality(self, api_client) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive search quality evaluation\"\"\"\n",
    "        \n",
    "        print(\"üéØ Starting Search Quality Evaluation...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for i, test_query in enumerate(self.test_queries, 1):\n",
    "            print(f\"\\nüîç Test Query {i}: '{test_query['query']}'\")\n",
    "            \n",
    "            # Make search request\n",
    "            search_result = await api_client.make_request(\n",
    "                endpoint='/search/smart',\n",
    "                method='POST',\n",
    "                data={\n",
    "                    'query': test_query['query'],\n",
    "                    'limit': 10,\n",
    "                    'enhance_query': True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if search_result.success:\n",
    "                try:\n",
    "                    response_data = json.loads(search_result.result) if isinstance(search_result.result, str) else search_result.result\n",
    "                    candidates = response_data.get('results', [])\n",
    "                    \n",
    "                    if candidates:\n",
    "                        # Evaluate each candidate\n",
    "                        candidate_scores = []\n",
    "                        for candidate in candidates:\n",
    "                            relevance_score = self.calculate_relevance_score(candidate, test_query)\n",
    "                            candidate_scores.append({\n",
    "                                'candidate_id': candidate.get('id'),\n",
    "                                'name': candidate.get('name'),\n",
    "                                'relevance_score': relevance_score,\n",
    "                                'api_score': candidate.get('score', candidate.get('relevance_score', 0)),\n",
    "                                'meets_threshold': relevance_score >= test_query['relevance_threshold']\n",
    "                            })\n",
    "                        \n",
    "                        # Calculate quality metrics\n",
    "                        avg_relevance = sum(cs['relevance_score'] for cs in candidate_scores) / len(candidate_scores)\n",
    "                        precision = sum(1 for cs in candidate_scores if cs['meets_threshold']) / len(candidate_scores)\n",
    "                        \n",
    "                        # API vs Manual score correlation\n",
    "                        api_scores = [cs['api_score'] for cs in candidate_scores if cs['api_score'] > 0]\n",
    "                        manual_scores = [cs['relevance_score'] for cs in candidate_scores if cs['api_score'] > 0]\n",
    "                        \n",
    "                        correlation = 0\n",
    "                        if len(api_scores) > 1:\n",
    "                            correlation = np.corrcoef(api_scores, manual_scores)[0, 1]\n",
    "                        \n",
    "                        query_evaluation = {\n",
    "                            'query': test_query['query'],\n",
    "                            'total_results': len(candidates),\n",
    "                            'avg_relevance_score': avg_relevance,\n",
    "                            'precision': precision,\n",
    "                            'threshold': test_query['relevance_threshold'],\n",
    "                            'score_correlation': correlation,\n",
    "                            'response_time': search_result.response_time,\n",
    "                            'candidate_scores': candidate_scores\n",
    "                        }\n",
    "                        \n",
    "                        evaluation_results.append(query_evaluation)\n",
    "                        \n",
    "                        print(f\"   ‚úÖ Results: {len(candidates)} candidates\")\n",
    "                        print(f\"   üìä Avg Relevance: {avg_relevance:.3f}\")\n",
    "                        print(f\"   üéØ Precision: {precision:.3f}\")\n",
    "                        print(f\"   üîó Score Correlation: {correlation:.3f}\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(f\"   ‚ùå No results returned\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error processing results: {e}\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"   ‚ùå Search failed: {search_result.error_message}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "# Initialize search quality evaluator\n",
    "quality_evaluator = SearchQualityEvaluator()\n",
    "print(\"üéØ Search quality evaluation framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run search quality evaluation\n",
    "async def run_search_quality_evaluation():\n",
    "    \"\"\"Execute search quality evaluation\"\"\"\n",
    "    \n",
    "    await benchmark.create_session()\n",
    "    \n",
    "    try:\n",
    "        quality_results = await quality_evaluator.evaluate_search_quality(benchmark)\n",
    "        return quality_results\n",
    "    finally:\n",
    "        await benchmark.close_session()\n",
    "\n",
    "# Execute quality evaluation\n",
    "search_quality_results = await run_search_quality_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search quality results\n",
    "def visualize_search_quality(quality_results: List[Dict[str, Any]]):\n",
    "    \"\"\"Visualize search quality evaluation results\"\"\"\n",
    "    \n",
    "    if not quality_results:\n",
    "        print(\"‚ö†Ô∏è No search quality data available\")\n",
    "        return\n",
    "    \n",
    "    # Create quality dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Relevance Score by Query',\n",
    "            'Precision vs Threshold',\n",
    "            'API vs Manual Score Correlation',\n",
    "            'Quality Score Distribution'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extract data\n",
    "    queries = [r['query'][:30] + '...' if len(r['query']) > 30 else r['query'] for r in quality_results]\n",
    "    avg_relevance = [r['avg_relevance_score'] for r in quality_results]\n",
    "    precisions = [r['precision'] for r in quality_results]\n",
    "    correlations = [r['score_correlation'] for r in quality_results]\n",
    "    \n",
    "    # 1. Relevance score by query\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=queries,\n",
    "            y=avg_relevance,\n",
    "            name='Avg Relevance',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Precision vs threshold\n",
    "    thresholds = [r['threshold'] for r in quality_results]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds,\n",
    "            y=precisions,\n",
    "            mode='markers',\n",
    "            marker=dict(size=12, color=avg_relevance, colorscale='viridis'),\n",
    "            name='Precision vs Threshold'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Score correlation\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=queries,\n",
    "            y=correlations,\n",
    "            name='Score Correlation',\n",
    "            marker_color='orange'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Quality score distribution\n",
    "    all_scores = []\n",
    "    for result in quality_results:\n",
    "        all_scores.extend([cs['relevance_score'] for cs in result['candidate_scores']])\n",
    "    \n",
    "    if all_scores:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=all_scores,\n",
    "                nbinsx=20,\n",
    "                name='Quality Distribution'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Search Quality Evaluation Results\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate overall quality metrics\n",
    "    overall_relevance = sum(avg_relevance) / len(avg_relevance) if avg_relevance else 0\n",
    "    overall_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    overall_correlation = sum(correlations) / len(correlations) if correlations else 0\n",
    "    \n",
    "    # Display summary\n",
    "    quality_summary = f\"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); color: white; padding: 20px; border-radius: 15px; margin: 15px 0;\">\n",
    "        <h4 style=\"margin-top: 0;\">üéØ Search Quality Summary</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 15px 0;\">\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>üìä Average Relevance</h5>\n",
    "                <p style=\"font-size: 24px; font-weight: bold;\">{overall_relevance:.3f}</p>\n",
    "                <p>{'‚úÖ Excellent' if overall_relevance >= 0.8 else '‚ö†Ô∏è Good' if overall_relevance >= 0.6 else '‚ùå Needs Improvement'}</p>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>üéØ Average Precision</h5>\n",
    "                <p style=\"font-size: 24px; font-weight: bold;\">{overall_precision:.3f}</p>\n",
    "                <p>{'‚úÖ High Quality' if overall_precision >= 0.8 else '‚ö†Ô∏è Moderate' if overall_precision >= 0.6 else '‚ùå Low Quality'}</p>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;\">\n",
    "                <h5>üîó Score Correlation</h5>\n",
    "                <p style=\"font-size: 24px; font-weight: bold;\">{overall_correlation:.3f}</p>\n",
    "                <p>{'‚úÖ Strong Alignment' if overall_correlation >= 0.7 else '‚ö†Ô∏è Moderate' if overall_correlation >= 0.5 else '‚ùå Weak Alignment'}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px; padding: 15px; background: rgba(255,255,255,0.1); border-radius: 10px;\">\n",
    "            <h5>üí° Quality Insights:</h5>\n",
    "            <ul style=\"margin: 10px 0;\">\n",
    "                <li>{'High relevance scores indicate accurate search results' if overall_relevance >= 0.7 else 'Consider improving search algorithm relevance'}</li>\n",
    "                <li>{'Good precision suggests well-tuned thresholds' if overall_precision >= 0.7 else 'Review relevance thresholds for better precision'}</li>\n",
    "                <li>{'Strong correlation between API and manual scores' if overall_correlation >= 0.6 else 'API scoring may need calibration with manual evaluation'}</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(quality_summary))\n",
    "\n",
    "# Visualize search quality results\n",
    "if search_quality_results:\n",
    "    visualize_search_quality(search_quality_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No search quality results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Performance Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_report(\n",
    "    response_time_stats: Dict[str, Any],\n",
    "    load_test_results: List[Dict[str, Any]],\n",
    "    search_quality_results: List[Dict[str, Any]]\n",
    ") -> str:\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# HR Resume Search MCP API - Performance Benchmark Report\n",
    "\n",
    "**Generated**: {timestamp}  \n",
    "**API Base URL**: {API_BASE_URL}  \n",
    "**Test Environment**: {os.getenv('ENVIRONMENT', 'development')}  \n",
    "**Test Duration**: {BENCHMARK_CONFIG['measurement_duration']}s per load level  \n",
    "**Max Concurrent Users**: {BENCHMARK_CONFIG['max_concurrent_users']}  \n",
    "\n",
    "## üìä Executive Summary\n",
    "\n",
    "| Metric | Value | Status |\n",
    "|--------|-------|--------|\n",
    "| Average Response Time | {response_time_stats.get('avg_response_time', 0):.3f}s | {'‚úÖ Pass' if response_time_stats.get('avg_response_time', 999) < BENCHMARK_CONFIG['response_time_threshold'] else '‚ùå Fail'} |\n",
    "| P95 Response Time | {response_time_stats.get('p95_response_time', 0):.3f}s | {'‚úÖ Good' if response_time_stats.get('p95_response_time', 999) < BENCHMARK_CONFIG['response_time_threshold'] * 1.5 else '‚ö†Ô∏è Attention'} |\n",
    "| Success Rate | {response_time_stats.get('success_rate', 0):.1f}% | {'‚úÖ Pass' if response_time_stats.get('success_rate', 0) >= (100 - BENCHMARK_CONFIG['error_threshold']) else '‚ùå Fail'} |\n",
    "| Peak Throughput | {max([r['stats']['successful_rps'] for r in load_test_results]) if load_test_results else 0:.1f} req/s | {'‚úÖ Excellent' if load_test_results and max([r['stats']['successful_rps'] for r in load_test_results]) > 50 else '‚ö†Ô∏è Moderate'} |\n",
    "| Search Quality Score | {sum([r['avg_relevance_score'] for r in search_quality_results]) / len(search_quality_results) if search_quality_results else 0:.3f} | {'‚úÖ High' if search_quality_results and sum([r['avg_relevance_score'] for r in search_quality_results]) / len(search_quality_results) > 0.7 else '‚ö†Ô∏è Moderate'} |\n",
    "\n",
    "## ‚ö° Response Time Analysis\n",
    "\n",
    "### Key Metrics\n",
    "- **Total Requests Tested**: {response_time_stats.get('total_requests', 0)}\n",
    "- **Average Response Time**: {response_time_stats.get('avg_response_time', 0):.3f}s\n",
    "- **Median Response Time**: {response_time_stats.get('median_response_time', 0):.3f}s\n",
    "- **95th Percentile**: {response_time_stats.get('p95_response_time', 0):.3f}s\n",
    "- **99th Percentile**: {response_time_stats.get('p99_response_time', 0):.3f}s\n",
    "- **Standard Deviation**: {response_time_stats.get('std_response_time', 0):.3f}s\n",
    "\n",
    "### Performance Assessment\n",
    "\"\"\"\n",
    "    \n",
    "    # Response time assessment\n",
    "    avg_time = response_time_stats.get('avg_response_time', 0)\n",
    "    if avg_time < 0.5:\n",
    "        report += \"- ‚úÖ **Excellent Response Times**: Average response time under 500ms\\n\"\n",
    "    elif avg_time < 1.0:\n",
    "        report += \"- ‚úÖ **Good Response Times**: Average response time under 1 second\\n\"\n",
    "    elif avg_time < 2.0:\n",
    "        report += \"- ‚ö†Ô∏è **Acceptable Response Times**: Average response time under 2 seconds\\n\"\n",
    "    else:\n",
    "        report += \"- ‚ùå **Poor Response Times**: Average response time exceeds 2 seconds\\n\"\n",
    "    \n",
    "    p95_time = response_time_stats.get('p95_response_time', 0)\n",
    "    if p95_time < 2.0:\n",
    "        report += \"- ‚úÖ **Good P95 Performance**: 95% of requests complete under 2 seconds\\n\"\n",
    "    elif p95_time < 5.0:\n",
    "        report += \"- ‚ö†Ô∏è **Moderate P95 Performance**: Some requests may be slow\\n\"\n",
    "    else:\n",
    "        report += \"- ‚ùå **Poor P95 Performance**: Significant latency for some requests\\n\"\n",
    "    \n",
    "    # Load testing results\n",
    "    if load_test_results:\n",
    "        peak_throughput = max([r['stats']['successful_rps'] for r in load_test_results])\n",
    "        peak_users = [r['concurrent_users'] for r in load_test_results if r['stats']['successful_rps'] == peak_throughput][0]\n",
    "        \n",
    "        # Find breaking point\n",
    "        breaking_point = None\n",
    "        for result in load_test_results:\n",
    "            if (result['stats']['success_rate'] < 95 or \n",
    "                result['stats'].get('avg_response_time', 0) > BENCHMARK_CONFIG['response_time_threshold']):\n",
    "                breaking_point = result['concurrent_users']\n",
    "                break\n",
    "        \n",
    "        report += f\"\"\"\n",
    "\n",
    "## üî• Load Testing Results\n",
    "\n",
    "### Scalability Metrics\n",
    "- **Peak Throughput**: {peak_throughput:.1f} requests/second\n",
    "- **Optimal Concurrent Users**: {peak_users}\n",
    "- **Breaking Point**: {breaking_point or 'Not reached within test limits'}\n",
    "- **Maximum Users Tested**: {max([r['concurrent_users'] for r in load_test_results])}\n",
    "\n",
    "### Load Test Summary\n",
    "\n",
    "| Users | Throughput (req/s) | Avg Response Time | Success Rate |\n",
    "|-------|-------------------|-------------------|-------------|\n",
    "\"\"\"\n",
    "        \n",
    "        for result in load_test_results:\n",
    "            stats = result['stats']\n",
    "            report += f\"| {result['concurrent_users']} | {stats['successful_rps']:.1f} | {stats.get('avg_response_time', 0):.3f}s | {stats['success_rate']:.1f}% |\\n\"\n",
    "        \n",
    "        # Scalability assessment\n",
    "        report += \"\\n### Scalability Assessment\\n\"\n",
    "        if peak_throughput > 100:\n",
    "            report += \"- ‚úÖ **Excellent Scalability**: High throughput achieved\\n\"\n",
    "        elif peak_throughput > 50:\n",
    "            report += \"- ‚úÖ **Good Scalability**: Moderate throughput capacity\\n\"\n",
    "        elif peak_throughput > 20:\n",
    "            report += \"- ‚ö†Ô∏è **Limited Scalability**: Consider optimization\\n\"\n",
    "        else:\n",
    "            report += \"- ‚ùå **Poor Scalability**: Significant performance limitations\\n\"\n",
    "        \n",
    "        if breaking_point:\n",
    "            report += f\"- ‚ö†Ô∏è **Performance Degradation**: Occurs at {breaking_point} concurrent users\\n\"\n",
    "        else:\n",
    "            report += f\"- ‚úÖ **Stable Performance**: No degradation up to {max([r['concurrent_users'] for r in load_test_results])} users\\n\"\n",
    "    \n",
    "    # Search quality results\n",
    "    if search_quality_results:\n",
    "        avg_relevance = sum([r['avg_relevance_score'] for r in search_quality_results]) / len(search_quality_results)\n",
    "        avg_precision = sum([r['precision'] for r in search_quality_results]) / len(search_quality_results)\n",
    "        avg_correlation = sum([r['score_correlation'] for r in search_quality_results]) / len(search_quality_results)\n",
    "        \n",
    "        report += f\"\"\"\n",
    "\n",
    "## üéØ Search Quality Analysis\n",
    "\n",
    "### Quality Metrics\n",
    "- **Average Relevance Score**: {avg_relevance:.3f}\n",
    "- **Average Precision**: {avg_precision:.3f}\n",
    "- **API-Manual Score Correlation**: {avg_correlation:.3f}\n",
    "- **Test Queries Evaluated**: {len(search_quality_results)}\n",
    "\n",
    "### Quality Assessment\n",
    "\"\"\"\n",
    "        \n",
    "        if avg_relevance >= 0.8:\n",
    "            report += \"- ‚úÖ **Excellent Search Relevance**: Highly accurate results\\n\"\n",
    "        elif avg_relevance >= 0.6:\n",
    "            report += \"- ‚úÖ **Good Search Relevance**: Generally accurate results\\n\"\n",
    "        else:\n",
    "            report += \"- ‚ùå **Poor Search Relevance**: Results need improvement\\n\"\n",
    "        \n",
    "        if avg_precision >= 0.8:\n",
    "            report += \"- ‚úÖ **High Precision**: Low false positive rate\\n\"\n",
    "        elif avg_precision >= 0.6:\n",
    "            report += \"- ‚ö†Ô∏è **Moderate Precision**: Some irrelevant results\\n\"\n",
    "        else:\n",
    "            report += \"- ‚ùå **Low Precision**: High false positive rate\\n\"\n",
    "        \n",
    "        if avg_correlation >= 0.7:\n",
    "            report += \"- ‚úÖ **Strong Score Alignment**: API scoring is well-calibrated\\n\"\n",
    "        elif avg_correlation >= 0.5:\n",
    "            report += \"- ‚ö†Ô∏è **Moderate Score Alignment**: Some calibration needed\\n\"\n",
    "        else:\n",
    "            report += \"- ‚ùå **Weak Score Alignment**: Scoring algorithm needs review\\n\"\n",
    "    \n",
    "    # Recommendations\n",
    "    report += \"\"\"\n",
    "\n",
    "## üí° Performance Optimization Recommendations\n",
    "\n",
    "### Immediate Actions\n",
    "\"\"\"\n",
    "    \n",
    "    # Response time recommendations\n",
    "    if response_time_stats.get('avg_response_time', 0) > BENCHMARK_CONFIG['response_time_threshold']:\n",
    "        report += \"- üö® **Critical**: Optimize response times - current average exceeds target\\n\"\n",
    "        report += \"  - Review database query performance\\n\"\n",
    "        report += \"  - Implement caching strategies\\n\"\n",
    "        report += \"  - Consider API endpoint optimization\\n\"\n",
    "    \n",
    "    # Success rate recommendations\n",
    "    if response_time_stats.get('success_rate', 0) < (100 - BENCHMARK_CONFIG['error_threshold']):\n",
    "        report += \"- üö® **Critical**: Address error rate - success rate below target\\n\"\n",
    "        report += \"  - Investigate failed requests\\n\"\n",
    "        report += \"  - Improve error handling\\n\"\n",
    "        report += \"  - Add retry mechanisms\\n\"\n",
    "    \n",
    "    # Scalability recommendations\n",
    "    if load_test_results:\n",
    "        peak_throughput = max([r['stats']['successful_rps'] for r in load_test_results])\n",
    "        if peak_throughput < 50:\n",
    "            report += \"- ‚ö° **Scalability**: Improve concurrent request handling\\n\"\n",
    "            report += \"  - Implement connection pooling\\n\"\n",
    "            report += \"  - Add horizontal scaling capabilities\\n\"\n",
    "            report += \"  - Optimize resource utilization\\n\"\n",
    "    \n",
    "    # Search quality recommendations\n",
    "    if search_quality_results:\n",
    "        avg_relevance = sum([r['avg_relevance_score'] for r in search_quality_results]) / len(search_quality_results)\n",
    "        if avg_relevance < 0.7:\n",
    "            report += \"- üéØ **Search Quality**: Improve search relevance\\n\"\n",
    "            report += \"  - Refine scoring algorithms\\n\"\n",
    "            report += \"  - Enhance query understanding\\n\"\n",
    "            report += \"  - Improve candidate matching logic\\n\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "\n",
    "### Long-term Improvements\n",
    "- üìä **Monitoring**: Implement continuous performance monitoring\n",
    "- üîÑ **Automation**: Add performance testing to CI/CD pipeline\n",
    "- üìà **Alerting**: Set up performance degradation alerts\n",
    "- üß™ **A/B Testing**: Test optimization strategies systematically\n",
    "- üíæ **Caching**: Implement advanced caching strategies\n",
    "- üîç **Search**: Enhance AI-powered search capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Report Generated by**: HR Resume Search MCP API Performance Benchmarking Suite  \n",
    "**Notebook**: `notebooks/performance_benchmarking.ipynb`  \n",
    "**Timestamp**: {timestamp}\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\n",
    "if response_time_stats and load_test_results:\n",
    "    performance_report = generate_performance_report(\n",
    "        response_time_stats,\n",
    "        load_test_results,\n",
    "        search_quality_results or []\n",
    "    )\n",
    "    \n",
    "    # Save report to file\n",
    "    report_filename = f\"performance_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(performance_report)\n",
    "    \n",
    "    print(f\"üìã Comprehensive performance report saved to: {report_filename}\")\n",
    "    \n",
    "    # Display report summary\n",
    "    display(Markdown(\"## üìã Performance Benchmark Report Summary\\n\\n\" + performance_report[:2000] + \"\\n\\n*Full report saved to file.*\"))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data to generate comprehensive report\")\n",
    "\n",
    "print(\"\\nüéâ Performance benchmarking suite completed successfully!\")\n",
    "print(f\"üìä Generated comprehensive analysis and report\")\n",
    "print(f\"üîç All performance metrics captured and analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary and Insights\n",
    "\n",
    "This comprehensive performance benchmarking suite provides:\n",
    "\n",
    "### ‚úÖ **Performance Analysis Features**\n",
    "- **‚ö° Response Time Analysis** - Detailed latency measurements across endpoints\n",
    "- **üî• Concurrent Load Testing** - Multi-user simulation with progressive load\n",
    "- **üéØ Search Quality Evaluation** - Relevance and accuracy scoring framework\n",
    "- **üìä System Resource Monitoring** - CPU, memory, and I/O tracking\n",
    "- **üìà Performance Visualization** - Interactive charts and dashboards\n",
    "- **üìã Comprehensive Reporting** - Detailed markdown reports with recommendations\n",
    "\n",
    "### üîß **Key Capabilities**\n",
    "- **Real-time Monitoring** - System resource tracking during tests\n",
    "- **Quality Metrics** - Precision, relevance, and correlation analysis\n",
    "- **Scalability Testing** - Breaking point identification and throughput analysis\n",
    "- **Performance Thresholds** - Automated pass/fail criteria evaluation\n",
    "- **Optimization Recommendations** - Data-driven improvement suggestions\n",
    "\n",
    "### üìä **Benchmarking Insights**\n",
    "- **Response Time Targets** - < 2s average, < 3s P95\n",
    "- **Throughput Goals** - > 50 req/s sustained\n",
    "- **Quality Standards** - > 0.7 relevance score, > 0.8 precision\n",
    "- **Error Tolerance** - < 5% error rate\n",
    "- **Scalability Requirements** - Handle 50+ concurrent users\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run benchmarks regularly to track performance trends\n",
    "2. Implement continuous performance monitoring\n",
    "3. Set up automated alerts for performance degradation\n",
    "4. Use insights to guide optimization efforts\n",
    "5. Integrate performance testing into CI/CD pipeline\n",
    "\n",
    "**Happy Benchmarking! üìäüöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}